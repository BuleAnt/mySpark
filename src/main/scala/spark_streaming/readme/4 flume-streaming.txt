SparkStreaming数据源Flume的安装配置及应用分析

一、什么是Flume?
flume 作为 cloudera 开发的实时日志收集系统，受到了业界的认可与广泛应用。
	Flume 初始的发行版本目前被统称为 Flume OG（original generation），属于 cloudera。
	但随着 FLume 功能的扩展，Flume OG 代码工程臃肿、核心组件设计不合理、核心配置不标准等缺点暴露出来，
	尤其是在 Flume OG 的最后一个发行版本 0.94.0 中，日志传输不稳定的现象尤为严重，为了解决这些问题，
	2011 年 10 月 22 号，cloudera 完成了 Flume-728，对 Flume 进行了里程碑式的改动：
		重构核心组件、核心配置以及代码架构，重构后的版本统称为 Flume NG（next generation）；
		改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。

	1. flume的特点：
		flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。
			支持在日志系统中定制各类数据发送方，用于收集数据;
			同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力 。
		flume的数据流由事件(Event)贯穿始终。
			事件Event是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，
			当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。
			你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。
			Sink负责持久化日志或者把事件推向另一个Source。
	　　
	2. flume的可靠性：
		当节点出现故障时，日志能够被传送到其他节点上而不会丢失。
		Flume提供了三种级别的可靠性保障，从强到弱依次分别为：
			end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），
			Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送），
			Besteffort（数据发送到接收方后，不会进行确认）。

	3. flume的可恢复性：
		还是靠Channel。推荐使用FileChannel，事件持久化在本地文件系统里(性能较差)。

	flume的一些核心概念：
		Agent使用JVM运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。
			1,Client 生产数据，运行在一个独立的线程。
			2,Source 从Client收集数据，传递给Channel
			3,Sink 从Channel收集数据，运行在一个独立线程。
			4,Channel 连接 sources 和 sinks ，这个有点像一个队列。
			5,Events 可以是日志记录、 avro 对象等。
		Flume以agent为最小的独立运行单位。一个agent就是一个JVM。单agent由Source、Sink和Channel三大组件构成，

		值得注意的是，Flume提供了大量内置的Source、Channel和Sink类型。
			不同类型的Source,Channel和Sink可以自由组合。组合方式基于用户设置的配置文件，非常灵活。比如：
				Channel可以把事件暂存在内存里，也可以持久化到本地硬盘上。
				Sink可以把日志写入HDFS, HBase，甚至是另外一个Source等等。
			Flume支持用户建立多级流，也就是说，多个agent可以协同工作，
		并且支持Fan-in、Fan-out、Contextual Routing、Backup Routes，这也正是NB之处。

三、Flume+Kafka+Spark Streaming应用场景：
	1、Flume集群采集外部系统的业务信息，将采集后的信息发生到Kafka集群，
		最终提供Spark Streaming流框架计算处理，流处理完成后再将最终结果发送给Kafka存储
		Flume-->Kafka Cluster-->Spark Steaming
		关于Kafka与Steaming数据传输见第四条
	2、Flume集群采集外部系统的业务信息，将采集后的信息发生到Kafka集群，
		最终提供Spark Streaming流框架计算处理，流处理完成后再将最终结果发送给Kafka存储，
		同时将最终结果通过Ganglia监控工具进行图形化展示。
	Flume-->Kafka Cluster-->Spark Steaming-->Kafka Cluster-->Ganglia
	3、我们要做：Spark streaming 交互式的360度的可视化，Spark streaming 交互式3D可视化UI；
		Flume集群采集外部系统的业务信息，将采集后的信息发送到Kafka集群，最终提供给Spark Streaming流框架计算处理，
		流处理完成后再将最终结果发送给Kafka存储，将最终结果同时存储在数据库（MySQL）、内存中间件（Redis、MemSQL）中，
		同时将最终结果通过Ganglia监控工具进行图形化展示，
	Flume-->Kafka Cluster-->Spark Steaming-->Kafka 存储+MySql(数据量小)（Redis(进一步比较大)、MemSQL）,更大的数据量使用HBase

四、Kafka数据写入Spark Streaming有二种方式：
	一种是Receivers，这个方法使用了Receivers来接收数据，Receivers的实现使用到Kafka高层次的消费者API，
		对于所有的Receivers，接收到的数据将会保存在Spark 分布式的executors中，
		然后由Spark Streaming启动的Job来处理这些数据；
		然而，在默认的配置下，这种方法在失败的情况下会丢失数据，为了保证零数据丢失，
		你可以在Spark Streaming中使用WAL日志功能，这使得我们可以将接收到的数据保存到WAL中（WAL日志可以存储在HDFS上），
		所以在失败的时候，我们可以从WAL中恢复，而不至于丢失数据。
	另一种是DirectAPI，即:产生数据和处理数据在同一台机器上
		Flume集群将采集的数据放到Kafka集群中，Spark Streaming会实时在线的从Kafka集群中通过DirectAPI拿数据，
		DirectAPI方式将会减少网络传输,但是由于在一台机器上有Driver和Executor,要要求这台机器足够强悍
	可以通过Kafka中的topic+partition查询最新的偏移量（offset）来读取每个batch的数据，
		即使读取失败也可再根据偏移量来读取失败的数据，保证应用运行的稳定性和数据可靠性。

	补充说明：
		1、Flume集群数据写入Kafka集群时可能会导致数据存放不均衡，
			即有些Kafka节点数据量很大、有些不大，后续会对分发数据进行自定义算法来解决数据存放不均衡问题。
		2、个人强烈推荐在生产环境下用DirectAPI，但是我们的发行版，会对DirectAPI进行优化，降低其延迟。
	总结：
	实际生产环境下，搜集分布式的日志以Kafka为核心。
	也有一些其他的消息组件:zeroMQ,activeMQ等,这些组件实际使用经验来说都不如kafka好

Flume推送数据到SparkStreaming案例实战和内幕源码解密
--------------------------------------------------------------
	Flume push to HDFS
	实例配置文件See : spool2hdfs.properties
	shell启动脚本See: flume_start.sh
	启动后向spoolDir路径下天加文件,Flume会自动将此文件，导入到Flume配置时指定的HDFS文件夹中
	并且spoolDir中放入的本地的文件添加了后缀.COMPLETED。

	补充说明：
	一般正常业务情况下，应该是把Flume的数据放到Kafka中，然后让不同的数据消费者去消费数据。
	如果要在Flume和Kafka两者间做选择的话，则要看业务中数据是否持续不断的产生。
		如果是这样，则应该选择Kafka。
		如果产生的数据时大时小，甚至有些时间没有数据，则没必要用Kafka，可以节省资源。

Flume推送数据到Spark Streaming实战
	现在我们不把Flume的数据导入到HDFS中，而是把数据推送到Spark Streaming中。

	Flume push -->被SparkSteaming监听host,port

	SparkSteaming监听到host:port的socket过来新数据后,就可以在线处理了
	flume配置See : avrosink.properties

	编写Spark Streaming应用的Java程序：
		See : Flume2Spark.java
		运行代码时候先启动SparkSteaming程序,然后在启动flume的agent
		如果先启动flume将会保错,这是应为flume的sink是push数据,对应的socket必须有监听才可以

	代码中用到了FlumeUtils。我们剖析一下代码中用到的FlumeUtils。
		FlumeUtils.createStream中设置默认参数
		 def createStream (
	          ssc: StreamingContext,
	          hostname: String,
	          port: Int,
	          storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2
	        ): ReceiverInputDStream[SparkFlumeEvent] = {
	        createStream(ssc, hostname, port, storageLevel, false)
	      }
	可以看到流处理默认的存储方式是:既在内存，又在磁盘中，同时做序列化，而且用两台机器。
	继续看调用的createStream方法：
		def createStream (
		  ssc: StreamingContext,
		  hostname: String,
		  port: Int,
		  storageLevel: StorageLevel,
		  enableDecompression: Boolean
		): ReceiverInputDStream[SparkFlumeEvent] = {
		val inputStream = new FlumeInputDStream[SparkFlumeEvent](
		    ssc, hostname, port, storageLevel, enableDecompression)
		inputStream
		}
	实际上返回的是FlumeInputDStream对象，而且事件是Flume所定义的事件SparkFlumeEvent。
	所以要注意，在以上Java代码做flatMap时，FlatMapFunction的输入类型必须是SparkFlumeEvent类型。
	再看看FlumeInputDStream的代码：
		  override def getReceiver(): Receiver[SparkFlumeEvent] = {
		    new FlumeReceiver(host, port, storageLevel, enableDecompression)
		  }
		}
	可以看到getReceiver返回的是用于接收数据的FlumeReceiver对象。再看FlumeReceiver：
		private def initServer() = {
	        if (enableDecompression) {
	          val channelFactory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(),
	                                                                 Executors.newCachedThreadPool())
	          val channelPipelineFactory = new CompressionChannelPipelineFactory()

	          new NettyServer(
	            responder,
	            new InetSocketAddress(host, port),
	            channelFactory,
	            channelPipelineFactory,
	            null)
	        } else {
	          new NettyServer(responder, new InetSocketAddress(host, port))
	        }
	      }
	可以发现Flume使用了Netty。如果搞分布式编程，要重视使用Netty。
	把以上的应用Spark Streaming的Java程序运行起来。确认Flume也在运行。
	我们找若干文件拷入spoor文件夹，那么在Java运行的控制台，可以发现Streaming 处理后的信息：
	说明Flume推送数据到了Spark Streaming，Spark Streaming对数据及时进行了处理。

	补充说明：
		使用Spark Streaming可以处理各种数据来源类型，
		如：数据库、HDFS，服务器log日志、网络流，其强大超越了你想象不到的场景，
		只是很多时候大家不会用，其真正原因是对Spark、spark streaming本身不了解。

三:SparkStreaming从Flume poll数据案例实战和内幕源码
--------------------------------------------------------------
推模式(Flume push SparkStreaming)与拉模式（SparkStreaming poll Flume）比较 ：
	采用推模式：推模式的理解就是Flume作为缓存，存有数据。监听对应端口，如果服务可以链接，就将数据push过去。
		(简单，耦合要低)，缺点是SparkStreaming 程序没有启动的话，Flume端会报错，同时可能会导致Spark Streaming 程序来不及消费的情况。
	采用拉模式：拉模式就是自己定义一个sink，SparkStreaming自己去channel里面取数据，根据自身条件去获取数据，稳定性好。

Flume poll 实战：
	1.Flume poll 配置
	See: sparksink.properties
	进入http://spark.apache.org/docs/1.6.1/streaming-flume-integration.html官网，
	加入依赖如下:
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-flume-sink_2.10</artifactId>
            <version>1.6.1</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>2.10.5</version>
        </dependency>
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-lang3</artifactId>
            <version>3.3.2</version>
        </dependency>
	对应jar包如下:
	spark-streaming-flume-sink_2.10-1.6.0.jar、scala-library-2.10.5.jar、commons-lang3-3.3.2.jar三个包：
	2、将下载后的三个jar包放入Flume安装lib目录：

三、编写代码：
	在Flume2Spark.java中只将JavaReceiverInputDStream<SparkFlumeEvent>  lines
	通过FlumeUtils.createPollingStream(jsc, "hadoop", 9999);
运行:
	使用sparksink的配置,启动flume的agent,并启动idea中的sparkStreaming的程序
	然后复制数据到poolDir中,或者使用代码脚本等生成文件到改目录下如 :
	运行脚本 echo2file.sh
	查看控制台输出

copy测试文件hellospark.txt到Flume flume-conf.properties配置文件中指定的/usr/local/flume/tmp/TestDir目录下：
隔24秒后可以在eclipse程序控制台中看到上传的文件单词统计结果。

四：源码分析：

	1、创建createPollingStream （FlumeUtils.scala ）：
	Flume输入流将会从Flume中pull数据,如果有数据的情况下
	2、参数配置：默认的全局参数，private 级别配置无法修改：

	3、创建FlumePollingInputDstream对象
	  def createPollingStream(
	      ssc: StreamingContext,
	      addresses: Seq[InetSocketAddress],
	      storageLevel: StorageLevel,
	      maxBatchSize: Int,
	      parallelism: Int
	    ): ReceiverInputDStream[SparkFlumeEvent] = {
	    new FlumePollingInputDStream[SparkFlumeEvent](ssc, addresses, maxBatchSize,
	      parallelism, storageLevel)
	  }
	FlumePollingInputDstream对象继承自ReceiverInputDstream中覆写getReciver方法，
	调用FlumePollingReciver接口
	private[streaming] class FlumePollingInputDStream[T: ClassTag](
		//.....
		  override def getReceiver(): Receiver[SparkFlumeEvent] = {
	        new FlumePollingReceiver(addresses, maxBatchSize, parallelism, storageLevel)
	      }
	    }
	5、ReceiverInputDstream 构建了一个线程池，设置为后台线程；
	4.并使用lazy和工厂方法创建线程和NioClientSocket
	（NioClientSocket底层使用NettyServer的方式）
	 FlumePollingReceiver(xxxx) extends Receiver[SparkFlumeEvent](storageLevel) with Logging {
	  //...
	  lazy val channelFactory =
	    new NioClientSocketChannelFactory(channelFactoryExecutor, channelFactoryExecutor)

		override def onStart(): Unit = {
	        // Create the connections to each Flume agent.
	        addresses.foreach(host => {
	          val transceiver = new NettyTransceiver(host, channelFactory)
	          val client = SpecificRequestor.getClient(classOf[SparkFlumeProtocol.Callback], transceiver)
	          connections.add(new FlumeConnection(transceiver, client))
	        })
	        for (i <- 0 until parallelism) {
	          logInfo("Starting Flume Polling Receiver worker threads..")
	          // Threads that pull data from Flume.
	          receiverExecutor.submit(new FlumeBatchFetcher(this))
	        }
	      }
	onStart根据并行度启动提交多个工作线程;工作线程从Flume Polling中pull 数据，实质上是从消息队列中获取数据
	即:FlumeBatchFetcher中的run()

	 class FlumeBatchFetcher(receiver: FlumePollingReceiver) extends Runnable {
	 def run(): Unit = {
         while (!receiver.isStopped()) {
           val connection = receiver.getConnections.poll()

	看FlumeBatchFetcher中的run()方法中的receiver.getConnections.poll()中的poll方法:
		  public E poll() {
		        final AtomicInteger count = this.count;
		        E x = dequeue();
		                c = count.getAndDecrement();
		        return x;
		    }
    发现dequeue出消息队列:进入dequeue中观察奇妙的现象出来了发现enqueue投递消息队列中我们发现调用enqueue方法的地方
	6、receiverExecutor 内部也是线程池；
		connections是指链接分布式Flume集群的FlumeConnection实体句柄的个数，线程拿到实体句柄访问数据。
	7、启动时创建NettyTransceiver，根据并行度(默认5个)循环提交FlumeBatchFetcher
	8、FlumeBatchFetcher run方法中从Receiver中获取connection链接句柄ack跟消息确认有关
	9、获取一批一批数据方法






















