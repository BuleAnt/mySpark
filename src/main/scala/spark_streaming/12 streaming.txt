1：kafka直接也可以监控一个文件夹，但是为什么我们采用flume的方式，通过flume把文件传给kafka，而不直接通过kafka去监控一个文件夹呢？

	1）flume可以搜集分布式的日志
	2）kafka直接读取文件，默认情况下，kafka要求文件格式是json格式。
	而数据很多情况下都不是json格式。这样就会崩溃。当然自定义一个读取器。
	所以kafka生产环境下很少去读文件，它就是直接收放消息的。

2：企业实际生产环境下初步架构图如下：

	1）下图是描述数据来源，如前端有高并发，每秒访问一百万，一千万之类的，
	用Nginx接收请求后，在后端用Tomcat或者apache之类的做负载均衡。
	数据来源:网站,app,设备等
	互联网:电商,社交网络的网站和App程序
	传统行业:金融,电信,医疗,农业,生产制造业
	例如说:在京东进行广告的推送,当我们点击的逛好的时候,此时肯定有日志log发送会Server
		或者说我们使用android,ios等中的app的时候,都设置有数据记录的关键点(埋点)
	如果是网站,经典的方式是通过js通过Ajax把日志传回到服务器上,
	如果移动app等一般是通过socket,其他的传感器或者工业设备都可以通过自己的通信协议把数据传回到服务端
	-->通过Ajax或者Socket或者其它通信协议-->
	[为了应对高并发访问,一般采用Nginx等作为server前端]
	-->Server的分布式集群来做负载均衡-->Tomcat/Apache

	2）Server中接收到请求路由后一般都会对每个请求在文件中写一条Log：
	可以自动配置Server写日志；
	也可以通过执行业务逻辑的过程来写日志

	Tomcat/Apache-->Logs Cluster:可以专门设置日志服务器集群,
	所有的Server和J2ee类型的业务逻辑在执行过程中产生的日志信息,都可以在后台同步到日志服务器集群中;

	3）用户生成的日志或者不同服务器生成的日志一般是比较零散的，
	企业中一般都会有Crontab等定时工具来通过日志整理工具来把当天的日志采集、合并和初步的处理成为一份日志文件，
	当然可以多份，分布式建议1份，然后发送到Flume监控目录中！！！
	当Flume发现有新的日志文件进来的时候，会按照配置把数据通过Channel来Sink到目的地，这里是Sink到Kafka集群中

	在企业:大数据生产环境核心技术:kafka,hdfs,spark,redis
	如果有第五个核心技术的化:kafka,hdfs,spark,redis,tachyon

	Logs Cluster-->Flume-->Kafka Cluster-->SparkStreaming-->Ganglia,DB/Redi

	在实际生产环境下大项目中用redis比较多,因为QPS可以非常高,尤其是在并发和实际要求比较高的场景特别有用
	(3000万/h,job:5亿)

	JDBC-->Java企业级别巨剑,获取DB数据并通过报表战士,提供营销运行,产品研发改进等

	6）上面kafka的方式是在线数据分析，下面是离线数据分析。
	日志上的数据当然可以如下图，直接放在hdfs上，但是实际生产环境下基本没有人直接把server上传的日志放在hdfs上。
	假设放在hdfs上，因为hdfs不断的收到数据，可以对数据进行如下初步的清洗：
	下面的分区表：每隔一小时或者一天一个目录。
	下面第3步，在已有表的基础上产生很多表，然后公司的业务人员分析人员基于这些目标表去写sql。
	执行他相应的分析需要

	Flume-->HDFS
	1.使用MapReduce作业对数据进行初步清洗,并写入新的HDFS文件中
	2.清晰后的数据一般导入到Hive数据仓库中,可以采用分区表
	3.通过Hive中的SQL在数据仓库的基础上进行ETL,此时ETL会把原始的数据生成很多张目标table


	上面数据仓库进行了多维度的ETL分析后，下面交给spark集群去处理。
	在企业生产环境下，大多数如果是离线数据分析，Spark处理数据都是基于Hive的，分析后的结果用于提升营业额，利润和市场占有率。
	而kafka则是在线数据分析























