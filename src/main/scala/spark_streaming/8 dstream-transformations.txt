SparkStreming中的Transformations和状态管理
1、SparkStreaming 中的Transformations
2、SparkStreaming 中的状态管理

一：SparkStreaming中的Transformation：
参考官方文档：
	http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html#transformations-on-dstreams
	1、DStream就是一个RDD之上的一个抽象，DStream和时间结合起来就不断的触发产生RDD的实例，
		可以说我们对DStream的操作就初步定义了对RDD的操作，
		只不过需要时间的间隔也就是internalBatch去激活这个模板，生成具体的RDD的实例和具体的job.
	2、我们鼓励Repartition，更多的是把更多的partition变成更少的partition，进行流的碎片的整理，
		我们不太鼓励把更少的partition变成更多的partion，因为会牵扯shuffle。
	3、DStream是离散流，离散流就没状态，
		除了计算每个时间间隔产生一个job，我们还有必要计算过去十分钟或者半个小时，
		所以这个时候我们需要维护这个状态。
	后台Spark提供了专门维护这个状态的函数updateStateByKey(func),即基于key，我们可以进行多个状态的维护。
		因为你可以把每一个时间间隔都做为一个状态，例如每一秒钟做为一个状态，我算下过去十分钟或者半个小时。值的更新就是通过传进来的func函数。

	map(func) 	Return a new DStream by passing each element of the source DStream through a function func.
	flatMap(func) 	Similar to map, but each input item can be mapped to 0 or more output items.
	filter(func) 	Return a new DStream by selecting only the records of the source DStream on which func returns true.
	repartition(numPartitions) 	Changes the level of parallelism in this DStream by creating more or fewer partitions.
	union(otherStream) 	Return a new DStream that contains the union of the elements in the source DStream and otherDStream.
	count() 	Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.
	reduce(func) 	Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream
		using a function func (which takes two arguments and returns one).
		The function should be associative so that it can be computed in parallel.
	countByValue() 	When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs
		where the value of each key is its frequency in each RDD of the source DStream.
	reduceByKey(func, [numTasks]) 	When called on a DStream of (K, V) pairs,
		return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function.
		Note: By default, this uses Spark's default number of parallel tasks (2 for local mode,
		and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping.
		You can pass an optional numTasks argument to set a different number of tasks.
	join(otherStream, [numTasks]) 	When called on two DStreams of (K, V) and (K, W) pairs,
		return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.
	cogroup(otherStream, [numTasks]) 	When called on a DStream of (K, V) and (K, W) pairs,
		return a new DStream of (K, Seq[V], Seq[W]) tuples.

	4、Transform：
		Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream.
	    This can be used to do arbitrary RDD operations on the DStream.
	    通过RDD到RDD的func方式对DStream进行transform,也就是说DStream中没有的相应的操作,那么可以转为RDD进行操作

        编程的逻辑是作用于RDD的
    Transform操作，允许任意的RDD和RDD的操作被应用在DStream上。
    他可以使这些RDD不容易暴露在DStreamAPI中。
        比如让两个batch产生join操作而不暴露在DStreamAPi中，然后你可以很容易的使用transform来做这。
        这将是非常有作用的，例如，能够将实时数据清理通过将输入的数据流和预先计算的垃圾信息过滤掉。
	updateStateByKey(func) 	Return a new "state" DStream where the state for each key is updated
		by applying the given function on the previous state of the key and the new values for the key.
		This can be used to maintain arbitrary state data for each key.
		通过指定的func对之前的DStream的k的用新的v的进行更新状态

	5、UpdateByKey
	UpdateStateByKey的操作，允许你维护任意的不断通过新的信息来更新的状态。使用这个函数你必须遵守两个步骤
		1）.定义一个状态：这个状态可以是任意的数据类型
		2）.定义一个状态更新函数：怎么样去使用从一个数据流中产生的旧的状态和新的状态来更新出一个状态。

	6、foreachRDD(func)
		The most generic output operator that applies a function, func, to each RDD generated from the stream.
		This function should push the data in each RDD to an external system,
			such as saving the RDD to files, or writing it over the network to a database.
		Note that the function func is executed in the driver process running the streaming application,
		and will usually have RDD actions in it that will force the computation of the streaming RDDs.

		mapWithState将流式的状态管理性能提高10倍以上,foreachRDD(func)中的函数func是作用于最后一个RDD,
		也就是结果RDD，如果RDD没有数据，就不需要进行操作，
		foreachRDD()可以将数据写在Redis/HBase/数据库/具体文件中，
		foreachRDD是在Driver程序中执行的，func就是action。
		官方非常好的案例:

			dstream.foreachRDD { rdd =>
			  rdd.foreachPartition { partitionOfRecords =>
			    // ConnectionPool is a static, lazily initialized pool of connections
			    val connection = ConnectionPool.getConnection()
			    partitionOfRecords.foreach(record => connection.send(record))
			    ConnectionPool.returnConnection(connection)  // return to the pool for future reuse
			  }
			}

	7、updateStateByKey
		1)在这里我们深入updateDateByKey的源码分析，首先进入PairDStreamFunctions.scala
			我们可以看到关于MapWithState官方的注释写的非常的清楚
		/**
		   * :: Experimental :://实验
		   * Return a [[MapWithStateDStream]] by applying a function to every key-value element of
		   * `this` stream, while maintaining some state data for each unique key. The mapping function
		   * and other specification (e.g. partitioners, timeouts, initial state data, etc.) of this
		   * transformation can be specified using [[StateSpec]] class. The state data is accessible in
		   * as a parameter of type [[State]] in the mapping function.
		   *
		   * Example of using `mapWithState`:
		   * {{{
		   *    // A mapping function that maintains an integer state and return a String
		   *    def mappingFunction(key: String, value: Option[Int], state: State[Int]): Option[String] = {
		   *      // Use state.exists(), state.get(), state.update() and state.remove()
		   *      // to manage state, and return the necessary string
		   *    }
		   *
		   *    val spec = StateSpec.function(mappingFunction).numPartitions(10)
		   *
		   *    val mapWithStateDStream = keyValueDStream.mapWithState[StateType, MappedType](spec)
		   * }}}
		   *
		   * @param spec          Specification of this transformation
		   * @tparam StateType    Class type of the state data
		   * @tparam MappedType   Class type of the mapped data
		   */
		  @Experimental
		  def mapWithState[StateType: ClassTag, MappedType: ClassTag](
		      spec: StateSpec[K, V, StateType, MappedType] //内部封装了函数,虽然语法上为变量
		    ): MapWithStateDStream[K, V, StateType, MappedType] = {
		    new MapWithStateDStreamImpl[K, V, StateType, MappedType](
		      self,
		      spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]]
		    )
		  }
		2)我么可以看到创建了MapWithStateDStreamImpl:
		3)我们可以看到updateStateByKey为实验性API

	  /**
	   * Return a new "state" DStream where the state for each key is updated by applying
	   * the given function on the previous state of the key and the new values of each key.
			//历史的基础上加上数据跟新,就是updateStateByKey
	   * org.apache.spark.Partitioner is used to control the partitioning of each RDD.
	   * @param updateFunc State update function. Note, that this function may generate a different
	   *                   tuple with a different key than the input key. Therefore keys may be removed
	   *                   or added in this way. It is up to the developer to decide whether to
	   *                   remember the partitioner despite the key being changed.
	   * @param partitioner Partitioner for controlling the partitioning of each RDD in the new
	   *                    DStream
	   * @param rememberPartitioner Whether to remember the paritioner object in the generated RDDs.
	   * @tparam S State type
	   */
	  def updateStateByKey[S: ClassTag](
	      updateFunc: (Iterator[(K, Seq[V], Option[S])]) => Iterator[(K, S)],
	      partitioner: Partitioner,
	      rememberPartitioner: Boolean
	    ): DStream[(K, S)] = ssc.withScope {
	     new StateDStream(self, ssc.sc.clean(updateFunc), partitioner, rememberPartitioner, None)
	  }

		4）接下来我们进入StateDStream

			class StateDStream[K: ClassTag, V: ClassTag, S: ClassTag](
	            parent: DStream[(K, V)],
	            updateFunc: (Iterator[(K, Seq[V], Option[S])]) => Iterator[(K, S)],
	            partitioner: Partitioner,
	            preservePartitioning: Boolean,
	            initialRDD : Option[RDD[(K, S)]]
	          ) extends DStream[(K, S)](parent.ssc) {

		5）在StateDStream我们可以看到利用computerPreviousRDD

		  override def compute(validTime: Time): Option[RDD[(K, S)]] = {
		    // Try to get the previous state RDD
		    getOrCompute(validTime - slideDuration) match {
		      case Some(prevStateRDD) => {    // If previous state RDD exists
		        // Try to get the parent RDD
		        parent.getOrCompute(validTime) match {
		          case Some(parentRDD) => {   // If parent RDD exists, then compute as usual
		            computeUsingPreviousRDD (parentRDD, prevStateRDD)
		          }

		6）我们进入computerPreviousRDD,可以发现此时的性能瓶颈。

		  private [this] def computeUsingPreviousRDD (
	        parentRDD : RDD[(K, V)], prevStateRDD : RDD[(K, S)]) = {
	        // Define the function for the mapPartition operation on cogrouped RDD;
	        // first map the cogrouped tuple to tuples of required type,
	        // and then apply the update function
	        val updateFuncLocal = updateFunc
	        val finalFunc = (iterator: Iterator[(K, (Iterable[V], Iterable[S]))]) => {
	          val i = iterator.map(t => {
	            val itr = t._2._2.iterator
	            val headOption = if (itr.hasNext) Some(itr.next()) else None
	            (t._1, t._2._1.toSeq, headOption)
	          })
	          updateFuncLocal(i)
	        }
	        val cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner)
	        val stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning)
	        Some(stateRDD)
	      }

	看到重点:
		cogroup是updateStateByKey性能的瓶颈，所有的老数据，过去的数据都要进行cogroup操作，
		即使新的数据pairedRDD只有一条记录，也要把所有的老记录都要进行cogroup操作。这时相当耗时的。
	理论上讲，只应对这条记录对应的key和历史的一批数据中对应的这个key进行更新操作就行了，
	而它更新全部的，99%的时间都是浪费和消耗。性能非常低。也会产生shuffle。

	而下面的MapWithState则只更新你必须要更新的，所以极大提升了性能。
	MapWithState只需要更新你必须更新的，没有必要更新所有的记录，官方宣传这个api会把流式的状态管理性能提升10倍以上。
	即:
	@Experimental
	  def mapWithState[StateType: ClassTag, MappedType: ClassTag](
	      spec: StateSpec[K, V, StateType, MappedType] //内部封装了函数,虽然语法上为变量
	    ): MapWithStateDStream[K, V, StateType, MappedType] = {
	    new MapWithStateDStreamImpl[K, V, StateType, MappedType](
	      self,
	      spec.asInstanceOf[StateSpecImpl[K, V, StateType, MappedType]]
	    )
	  }
