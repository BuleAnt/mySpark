SparkStreaming 实现广告计费系统中在线黑名单过滤实战
1、在线黑名单过滤实现解析
2、SparkStreaming实现在线黑名单过滤

一、在线黑名单过滤实现解析：
流式处理是现代数据处理的主流,各种电子商务网站,搜索引擎等网站等,都需要做流式比如，
	通过用户的点击和购买来推断出用户的兴趣爱好，后台能实时计算，
	这是比较重要的，给用户推荐最好的商品等，推荐更新的信息，给用户更好的服务。

Spark Streaming就是Spark Core上的一个应用程序。
	Spark Streaming中数据是不断的流进来，流进来的数据不断的生成Job，不断的提交给集群去处理，
	要是想更清晰的看到数据流进来，更清晰的看到数据被处理，只要把Batch Interval修改的足够大，就可以看到了，
	对于想理解内部的运行过程，排除错误等，都是很有必要的。

广告计费系统，是电商必不可少的一个功能点。
	为了防止恶意的广告点击(假设商户A和B同时在某电商做了广告，A和B为竞争对手，
	那么如果A使用点击机器人进行对B的广告的恶意点击，那么B的广告费用将很快被用完)，
	必须对广告点击进行黑名单过滤。
	黑名单的过滤可以是ID，可以是IP等等，黑名单就是过滤的条件，
利用SparkStreaming的流处理特性，可实现实时黑名单的过滤实现。
	可以使用leftouter join 对目标数据和黑名单数据进行关联，将命中黑名单的数据过滤掉。

	Code See: OnlineBlackListFilter.scala


通过Spark Streaming的window操作实战模拟热点搜索词案例实战
----------------------------------------------------------------------------------
1、在线热点搜索词实现解析
2、SparkStreaming 利用reduceByKeyAndWindow实现在线热点搜索词实战
一：在线热点搜索词实现解析

背景描述：
	在社交网络（例如微博），电子商务（例如京东），热搜词（例如百度）等
	人们核心关注的内容之一就是我所关注的内容中，大家正在最关注什么或者说当前的热点是什么，
	这在市级企业级应用中是非常有价值，
	例如我们关心过去30分钟大家正在热搜什么，并且每5分钟更新一次，这就使得热点内容是动态更新的，当然更有价值。
	Yahoo（是Hadoop的最大用户）被收购，因为没做到实时在线处理.
	实现技术：Spark Streaming（在线批处理）
	提供了滑动窗口的奇数来支撑实现上述业务背景，我们可以使用reduceByKeyAndWindow操作来做具体实现

	window操作:
		我们知道在SparkStreaming中可以设置batchInterval，让SparkStreaming每隔batchInterval时间提交一次Job，
		假设batchInterval设置为5秒，那如果需要对1分钟内的数据做统计，该如何实现呢？
		SparkStreaming中提供了window的概念。我们看下图：
		http://spark.apache.org/docs/1.6.1/img/streaming-dstream-window.png
	参考官方doc
	http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html#transformations-on-dstreams
		官网给的例子每个2秒钟更新过去3秒钟的内容，3秒钟算一下，5秒钟算一下，这里的3秒钟是一个时间窗口。
			如果每5分钟计算一次前30分钟的数据,这会重复25分钟数据,这里处理正常每次对过去30分钟计算外,
			可以通过checkpoint调优,基于上一次结果去除最后5分钟,加上最新5分钟的数据进行计算
		window可以包含多个batchInterval(例如5秒)，但是必须为batchInterval的整数倍例如1分钟。
		另外window可以移动，称之为滑动时间间隔，它也是batchInterval的整数倍,例如10秒。
		一般情况滑动时间间隔小于window的时间长度，否则会丢失数据。

	SparkStreaming提供了如下与window相关的方法：

		window(windowLength, slideInterval)//通过window长度,滑动间隔,对滑动窗口的数据进行计算
			Return a new DStream which is computed based on windowed batches of the source DStream.
		countByWindow(windowLength, slideInterval)
			对Window中的数据进行count操作
		reduceByWindow(func, windowLength, slideInterval)
			对每个window进行reduce操作
		reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])
			对每个window中DStream进行reduceByKey操作
		reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])
			这个对上一个reduceByKeyAndWindow函数重载,计算方式是基于上一个Window加上新的减去旧的
		countByValueAndWindow(windowLength, slideInterval, [numTasks])
			对每个Window进行countByValue操作

二、SparkStreaming 实现在线热点搜索词实战
	1、经过分析我们采用reduceByKeyAndWindow的方法，reduceByKeyAndWindow方法分析如下：

	从代码上面来看， 入口为：
	reduceByKeyAndWindow(_+_, _-_, Duration, Duration)
	一步一步跟踪进去， 可以看到实际的业务类是在ReducedWindowedDStream 这个类里面：
	代码理解就直接拿这个类来看了：

	val mergedValuesRDD = cogroupedRDD.asInstanceOf[RDD[(K, Array[Iterable[V]])]]
	      .mapValues(mergeValues)
	先计算oldRDD 和newRDD
	//currentWindow 就是以当前时间回退一个window的时间再向前一个batch 到当前时间的窗口 代码里面有一个图很有用：
	我们要计算的new rdd就是15秒-25秒期间的值， oldRDD就是0秒到10秒的值， previous window的值是1秒 - 15秒的值
	然后最终结果是 重复区间（previous window的值 - oldRDD的值） =》 也就是中间重复部分， 再加上newRDD的值， 这样的话得到的结果就是10秒到25秒这个时间区间的值
		// 0秒                  10秒     15秒                25秒
		//  _____________________________
		// |  previous window   _________|___________________
		// |___________________|       current window        |  --------------> Time
		//                     |_____________________________|
		//
		// |________ _________|          |________ _________|
		//          |                             |
		//          V                             V
		//       old RDDs                     new RDDs
		//
	reduceByWindow(reduceFunc, windowDuration, slideDuration) 代码：
	可以看到他做了两次reduce， 第一次对整个self做一次reduce， 然后截取时间区间， 对结果再做一次reduce。

	第一点： 对整个self做reduce会比较慢， 因为self都是相对比较大的集合。
	第二点：进行了两次reduce ，源码如下：

	def reduceByWindow(
	    reduceFunc: (T, T) => T,
	    windowDuration: Duration,
	    slideDuration: Duration
	  ): DStream[T] = ssc.withScope {
	  this.reduce(reduceFunc).window(windowDuration, slideDuration).reduce(reduceFunc)
	}
	如果我们看：reduceByWindow(reduceFunc, invReduceFunc, windowDuration, slideDuration)
	实际上他是调用了效率非常高的reduceByKeyAndWindow(reduceFunc, invReduceFunc, windowDuration, slideDuration, numPartitions, filterFunc) 方法
	这样的话其实他只对newRDDs和oldRDDs做reduce， 由于这两个RDDs都非常小， 可以想象效率是非常高的
	def reduceByWindow(
	    reduceFunc: (T, T) => T,
	    invReduceFunc: (T, T) => T,
	    windowDuration: Duration,
	    slideDuration: Duration
	  ): DStream[T] = ssc.withScope {
	    this.map(x => (1, x))
	        .reduceByKeyAndWindow(reduceFunc, invReduceFunc, windowDuration, slideDuration, 1)
	        .map(_._2)
	}

	如果看reduceByKeyAndWindow的话， 情况也是一样， 一个是执行：

	self.reduceByKey(reduceFunc, partitioner)
	        .window(windowDuration, slideDuration)
	        .reduceByKey(reduceFunc, partitioner)
	而另外一个确是在已有的window值基础上做了简单的加加减减

	宗上， 从效率上面考虑， 我们应该尽量使用包含invReduceFunc的方法， 同样情况下摒弃只有reduceFunc的方法
	2、我们案例代码如下：
	See :OnlineHottestItems.scala

	3、将程序打包运行到集群上观察结果：
	4、接下来我们使用reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks]) 这个函数，来实现增量的计算。

		使用这个函数，必须进行Checkpoint。代码如下
		ssc.checkpoint("target/checkpoints/")
		val ItemCount = ItemPairs.reduceByKeyAndWindow((v1:Int,v2:Int)=> v1+v2,(v1:Int,v2:Int)=> v1-v2,Seconds(60),Seconds(10))



