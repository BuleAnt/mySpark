一: Spark SQL案例综合
	1，Spark SQL案例分析join操作
	2，通过Java和Scala实现案例
	一个DF读取json文件,格式如下peoples.json
{"name":"Michael","score":98}
{"name":"Andy","score":95}
{"name":"Justin","score":68}
	另外一个表的数据不在一个文件里，采用代码编程。
	讲两个DF进行join,是通过转化为RDD的join,然后在转回DF
	See: JSQLwithJoin.java/SQLwithJoin.scala
	实际场景的数据来源可能有多重不同的数据来源，可能来自另外一个计算框架。

二: Spark SQL通过JDBC操作Mysql
	1，Spark SQL操作关系型数据库的意义
	2，Spark SQL操作关系型数据库实战

	使用Spark通过JDBC操作数据库
	Spark SQL可以通过JDBC从传统的关系型数据库中读写数据，读取数据后直接生成的是DataFrame。
	然后再加上借助于Spark内核的丰富的API来进行各种操作。
	从计算数据规模的角度去讲，集群并行访问数据库数据。

	1.通过format（“jdbc”） 的方式说明sparksql操作的数据通过jdbc获得
	jdbc 后端一般是数据库例如mysql 。oracle
	2.通过DataFrameReader的option的方法把方位的数据库的信息传递进去
		url：代表数据库的jdbc链接地址
		datable 具体指哪个数据库
	3. dirver 部分是是sparksql访问数据库的具体的驱动完整的包名和类名
	4. 关于jdbc 的驱动jar，可以放在spark的lib 目录下，也可以在使用sparksubmit的使用指定的jar

	在实际的企业级开发环境中我们如果数据库中数据规模特别大，
	例如10亿条数据，此时采用传统的db 去处理的话，一般需要对数据分成很多批次处理例如分成100批
	（首受限于单台server的处理能力）且实际处理可能会非常复杂，通过传统的J2ee 等基石很难或者很不方便实现处理方法，
	此时使用sparksql获得数数据库中的数据并进行分布式处理就可以非常好的解决该问题，
	但是sparksql 加载数据需要时间，所以一边会在sparksql和db 之间加一个缓冲层
	例如中间使用redis，可以把spark的处理速度提高甚至45倍。

	1.当dataframe要把通过spark sql、core、ml等复杂操作后的数据写入数据库的时候 首先是权限的问题，必须
	确保数据库授权了当前操作spark sql的用户
	2.Dataframe要写数据到db 的时候，一般都不可以直接写进去，而是要转成RDD，通过RDD写数据到db中
	Java版本：See:JJDBC2MySql.java
create table stu_age (name varchar(20) not null primary key,age tinyint);
create table stu_score (name varchar(20) not null primary key,score int);
insert into stu_age values('zhangsan',17);
insert into stu_age values('lisi',18);
insert into stu_score values('zhangsan',71);
insert into stu_score values('lisi',81);

create table  if not exists student(name varchar(20)not null primary key,age tinyint,score int)
insert into result values('string',1,2);

三: Spark SQL通过Hive数据源
	1，Spark SQL操作Hive解析
	2，Spark SQL操作Hive实战

	在目前企业级大数据Spark开发时候绝大多手情况下是采用Hive作为数据仓库,
	Spark提供了Hive的支持功能,Spark通过HiveContext可直接操作Hive中数据
		1.基于HiveContext我们可以使用sql/hql两种方式编写sql语句对hive进行操作,包括
			创建表,删除表,向表里导入数据,以及用sql语法构造各种sql语句对表中数据进行操作
		2.直接通过saveAsTable的方式把DaraFrame中的数据保存到Hive数据仓库中
		3.直接通过HiveContext.table方法来直接加载Hive中的表而生成DataFrame

	打包idea方法:
	File->Project Structure-->Artifacts-->选择from Module..本项目,
	然后删除所有依赖jar包,只保留本项目mySpark compile output,选择主类,必须是class,object不能选择
	指定输出路径,jar名称后保存,Build-->Build Artifacts
	启动环境:hive –service metastore &
	脚本See: hiveContext.sh
	其他参考:
		https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark

四: SparkSQL内置函数
	1 SparkSQL内置函数解析
	2 SparkSQL内置函数实战

	SparkSQL的DataFrame引入了大量的内置函数，这些内置函数一般都有CG（CodeGeneration）功能，这样的函数在编译和执行时都会经过高度优化。
	问题：SparkSQL操作Hive和Hive on Spark一样吗？
		不一样。
		SparkSQL操作Hive只是把Hive当作数据仓库的来源，而计算引擎就是SparkSQL本身。
		Hive on spark是Hive的子项目，Hive on Spark的核心是把Hive的执行引擎换成Spark。
		众所周知，目前Hive的计算引擎是Mapreduce，因为性能低下等问题，所以Hive的官方就想替换这个引擎。
	SparkSQL操作Hive上的数据叫Spark on Hive，而Hive on Spark依旧是以Hive为核心，只是把计算引擎由MapReduce替换为Spark
	Spark官网上DataFrame 的API Docs：http://spark.apache.org/docs/1.6.1/api/scala/index.html#org.apache.spark.sql.DataFrame

使用Scala开发集群运行的Spark WordCount程序
	使用Spark SQL中的内置函数对数据进行分析，Spark SQL API不同的是，
	DataFrame中的内置函数操作的结果是返回一个Column对象，
	而DataFrame天生就是"A distributed collection of data organized into named columns.",
	这就为数据的复杂分析建立了坚实的基础并提供了极大的方便性，
		例如说，我们在操作DataFrame的方法中可以随时调用内置函数进行业务需要的处理，
		这之于我们构建附件的业务逻辑而言是可以极大的减少不必须的时间消耗（基于上就是实际模型的映射），
		让我们聚焦在数据分析上，这对于提高工程师的生产力而言是非常有价值的

	Spark 1.5.x开始提供了大量的内置函数，例如agg：
	def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
		groupBy().agg(aggExpr, aggExprs : _*)
	}
	还有max、mean、min、sum、avg、explode、size、sort_array、day、to_date、abs、acros、asin、atan
	总体上而言内置函数包含了五大基本类型：
		1, 聚合函数，例如countDistinct、sumDistinct等；
		2, 集合函数，例如sort_array、explode等
		3, 日期、时间函数，例如hour、quarter、next_day
		4, 数学函数，例如asin、atan、sqrt、tan、round等；
		5, 开窗函数，例如rowNumber等
		6, 字符串函数，concat、format_number、rexexp_extract
		7, 其它函数，isNaN、sha、randn、callUDF

	第1步：创建Spark的配置对象SparkConf，设置Spark程序的运行时的配置信息，
		例如说通过setMaster来设置程序要链接的Spark集群的Master的URL,如果设置
		为local，则代表Spark程序在本地运行，特别适合于机器配置条件非常差（例如只有1G的内存）的初学者
	第2步：创建SparkContext对象
		SparkContext是Spark程序所有功能的唯一入口，无论是采用Scala、Java、Python、R等都必须有一个SparkContext
		SparkContext核心作用：
		初始化Spark应用程序运行所需要的核心组件，包括DAGScheduler、TaskScheduler、SchedulerBackend
		同时还会负责Spark程序往Master注册程序等
		SparkContext是整个Spark应用程序中最为至关重要的一个对象
	第3步：模拟电商访问的数据，实际情况会比模拟数据复杂很多，最后生成RDD
	第4步：根据业务需要对数据进行预处理生成DataFrame，
		要想把RDD转换成DataFrame，需要先把RDD中的元素类型变成Row类型
		于此同时要提供DataFrame中的Columns的元数据信息描述
	第5步：使用Spark SQL提供的内置函数对DataFrame进行操作
		特别注意：内置函数生成的Column对象且自定进行CG；
	Code See:SQL2Hive.scala
	导出后使用shell/hive_join.sh在集群运行(idea可以按照shell插件直接执行)

五: Spark SQL窗口函数
	窗口函数是Spark内置函数中最有价值的函数，因为很多关于分组的统计往往都使用了窗口函数。
	Spark SQL supports three kinds of window functions:
		ranking functions
		analytic functions
		aggregate functions
	Table 1. Window functions in Spark SQL (see Introducing Window Functions in Spark SQL)
		Ranking functions
			SQL         DataFrame API
			RANK            rank
			DENSE_RANK      dense_rank
			PERCENT_RANK    percent_rank
			NTILE           ntile
			ROW_NUMBER      row_number
		Analytic functions
			SQL         DataFrame API
			CUME_DIST       cume_dist
			LAG             lag
			LEAD            lead
	For aggregate functions, you can use the existing aggregate functions as window functions,
	    e.g. sum, avg, min, max and count.
	You can mark a function window by OVER clause after a function in SQL,
	    e.g. avg(revenue) OVER (…​) or over method on a function in the Dataset API, e.g. rank().over(…​).
	When executed, a window function computes a value for each row in a window.
	窗口函数中最重要的是row_number。row_bumber是对分组进行排序，所谓分组排序就是说在分组的基础上再进行排序。
	Coding:See SQLWindowFunction.scala
	导出后使用shell/row_number.sh在集群运行(idea可以按照shell插件直接执行)

六：Spark UDF和UDAF
	UDF:用户自定义函数,实现上就是普通scala函数
	UDAF:USER DEFINE AGGREGATE FUNCTION用户自定义聚合函数
	实质上,例如UDF会被SparkSQL中的Catalyst封装成为Expression,
	最终通过eval方法来计算输入的数据Row(此处的Row和DataFrame中的Row没关系)
	Spark SQL UDF和UDAF解决的问题,已有内置函数不足的问题,UDFA可以对数据集合进行操作,Aggregate:聚合
	See: SQLUDFUDAF.scala

七: Spark SQL Thrift Server JDBC编程
	目标：希望操作人员能web操作简单的SQL，通过Thrift Server可以从Web的角度来访问Server
	企业经典架构流程：通过JDBC/ODBC --> Thrift Server --> Spark SQL --> Hive数据仓库
	如果使用实时性要求不是特别高的情况下,一般可以取代传统数据库为后台的数据处理系统
	官方doc:
	http://spark.apache.org/docs/1.6.1/sql-programming-guide.html#running-the-thrift-jdbcodbc-server
	thrift server的启动顺序:先在hive的server端启动Hive Metastore Server服务:
	hive --service metastore &
	然后在spark集群启动thriftserver,可以指定一下参数:spark集群master,hive节点的ip和host
	sbin/start-thriftserver.sh
	--master spark://hadoop:7077
	--hiveconf hive.server2.thrift.port=10000
	--hiveconf hive.server2.bind.host=hadoop
	通过shell脚本和log日志发现其内部执行的是org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
	其实是启动了hive中的HiveThriftServer2
		但是与hive启动的thrift不同(即hive中命令启动的thrift: hive --service hiveserver2服务),
		关于两中thrift总结来说,就是通过启动thrift使用的是它自己的执行引擎,
		而beeline只是一个连接thrift server的client,本质是一样的,连接到谁的thrift就会使用谁的执行引擎
	spark是启动了一个spark App,内部通过spark的rdd执行sql
		另外关于权限问题:
		hive thrift server如果username输入的不是hadoop集群的用户(权限700),将在执行查新执行mr的时候报错:
		org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException):
		Permission denied: user=anonymous, access=EXECUTE,
		inode="/tmp/hadoop-yarn/staging/anonymous/.staging/job_1475653852498_0002":
		hadoop:supergroup:drwx------
		除了在连接时候指定外可以在hive-site中指定,参考:
		http://www.2cto.com/database/201309/244658.html
	如此成功启动了hive thrift,然后可以打开一个beeline,相当于一个客户端,输入链接命令链接到hive
	bin/beeline
beeline> !connect jdbc:hive2://localhost:10000
INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:10000
Connected to: Spark SQL (version 1.6.1)
Driver: Spark Project Core (version 1.6.1)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://localhost:10000>
	默认账户密码随便输入都可以,企业级别一般会设置,出现以上提示表示链接成功,如此可以像在hive的shell中一样操作hive
	随便执行一个查询操作比如:select count(name) from spark.scores;
	打开Application Spark-Shell的webUI:http://192.168.0.3:4040/jobs/
	能够查看到其在spark中执行的是rdd
	beeline的退出方式!q或者!quit/!exit等

	HiveThriftServer2代码:
	/**
	 * The main entry point for the Spark SQL port of HiveServer2.  Starts up a `SparkSQLContext` and a
	 * `HiveThriftServer2` thrift server.
	 */
	SparkSQLEnv.init()--> new SparkConf(loadDefaults = true)
		new HiveContext(sparkContext)...
	val server = new HiveThriftServer2(SparkSQLEnv.hiveContext)
	这里就是说spark中执行该sql,不是通过hadoop的mr执行,是通过rdd,这就会比hive快


















rdd-->DF
http://www.it1352.com/220642.html
http://www.myexception.cn/other/1961405.html
https://my.oschina.net/cjun/blog/655226
http://blog.sina.com.cn/s/blog_156a5741a0102wgkm.html