Spark SQL 实战
在$SPARK_HOME/conf下复制一个hive-size.xml,并添加一下配置
    <property>
		<name>hive.metastore.uris</name>
		<value>thrift://hadoop:9083</value>
		<description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
	</property>

    先启动$HADOOP_HOME/sbin/start-all.sh
    启动hive的service
        sudo service mysqld start
        $HIVE_HOME/bin/hive --service metastore &
        或者hive –service metastore >metastore.log 2>& 1&
    启动spark然后,启动一个spark-shell

------------------------------------------------------------------
操作如下:
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.sql("use hive")
//返回一个DataFrame
hiveContext.sql("show tables").collect.foreach(println)
hiveContext.sql("select count(*) from sogouQ1").collect.foreach(println)
必须放到Driver上才行。
hiveContext.sql("select count(*) from sogouQ3 where WEBSITE like '%baidu%'").collect.foreach(println)
hiveContext.sql("select count(*) from sogouQ3 where WEBSITE like '%baidu%' and S_SEQ=1 and C_SEQ=1").collect.foreach(println)
正常的情况下，比Hive SQL快20倍。

-------------------------------------------------------------------
参考官方doc练习,启动Spark集群
val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val df = sqlContext.read.json("examples/src/main/resources/people.json")
// Displays the content of the DataFrame to stdout
df.show()
df.printSchema()
df.select("name").show()
df.select(df("name"), df("age") + 1).show()
df.filter(df("age") > 21).show()
df.groupBy("age").count().show()

sqlContext
hiveContext
说明可以创建很多hiveContext，hiveContext连的是数据仓库，作为程序本身，
例如Spark中的Job可以并行运行，所以可以有多个hiveContext实例。查询数据级别有多个实例很正常。
val df = sqlContext.read.josn(“/library/examples/src/main/resources/people.json”)
把数据读取进来，知道数据的基本信息，json数据是key-value级别。


使用Java编写DataFrame   See:main/java/spark_sql/DataFrameOps.java
使用Scala编写DataFrame  See:main/scala/spark_sql/DataFrameOpsScala.scala

maven 打包参考http://www.cnblogs.com/yjmyzz/p/4694219.html
导包后,使用一下命令提交:
spark-submit --class spark_sql.DataFrameOpsScala --master spark://hadoop:7077 /home/hadoop/Document/export/xx.jar
业界为了统一，提交的过程，加上一个 --file,指定额外的配置文件,不用也没问题,主要是没用Hive的功能。
--files $SPARK_HOME/conf/hive-site.xml
或写入到一个shell脚本中:

/usr/local/spark/spark-1.6.0-bin-hadoop2.6/bin/spark-submit
--class com.dt.spark.sql.DataFrameOps
--files $HIVE_HOME/conf/hive-site.xml
--driver-class-path $SPARK_HOME/lib/mysql-connector-java-5.1.35-bin.jar
--master spark://Master:7077 /home/hadoop/Document/export/WordCount.jar

这样可以配置Hive的不同数据来源，如果不配置，就找Hive默认的配置信息。
数据仓库可能有不同的数据仓库。这么写是有道理的。Spark SQL当然可以用HQL。
如果Json数据嵌套比较复杂，先做一下ETL。

使用Java和Scala实战RDD和DataFrame
-------------------------------------------------------
	1，RDD和DataFrame转换的重大意义
	2，使用Java实战RDD与DataFrame转换
	3，使用Scala实战RDD与DataFrame转换

一：RDD和DataFrame转换的重大意义
    这种转换是相互的。RDD可以接上数据库，文件系统等数据来源。
    可以通过java的bean或者case class来获取RDD数据的schema,创建DataFrame。
        Java 的bean不能有嵌套结构和复杂数据结构。
        Scala本身没有这种限制，Scala可以做类型推断和隐式转换等。
    coding See:
        RDD2DataFrameByReflectionJava.java
	    RDD2DataFrameByReflectionScala.scala
    另外一种更加常见情况，创建DataFrame时，并不知道RDD的数据，需要动态获取schema
    使用Java和Scala实战RDD和DataFrame动态转换操作
	1，RDD和DataFrame的动态转换
    上一讲，知道了原数据的信息，是一种非动态的转换方式。
    只有在运行的时候才知道数据的列的信息，就需要动态转换了，这是生产环境下更加常见的情况。
    提前知道列的信息，这种情况比较少见，还有就是业务发生改变，列的信息也会改变。
    需要动态的RDD和DataFrame的动态转换。
    Job启动的开销，IO的开销，JVM的复用，还有钨丝计划。SparkSQL比Hive的优势很大。
    Java版本See: RDD2DataFrameByProgrammaticallyJava.java







