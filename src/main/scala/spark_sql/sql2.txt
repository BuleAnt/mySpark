Spark SQL 实战
在$SPARK_HOME/conf下复制一个hive-size.xml,并添加一下配置
    <property>
		<name>hive.metastore.uris</name>
		<value>thrift://hadoop:9083</value>
		<description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
	</property>

    先启动$HADOOP_HOME/sbin/start-all.sh
    启动hive的service
        sudo service mysqld start
        $HIVE_HOME/bin/hive --service metastore &
        或者hive –service metastore >metastore.log 2>& 1&
    启动spark然后,启动一个spark-shell

------------------------------------------------------------------
操作如下:
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.sql("use hive")
//返回一个DataFrame
hiveContext.sql("show tables").collect.foreach(println)
hiveContext.sql("select count(*) from sogouQ1").collect.foreach(println)
必须放到Driver上才行。
hiveContext.sql("select count(*) from sogouQ3 where WEBSITE like '%baidu%'").collect.foreach(println)
hiveContext.sql("select count(*) from sogouQ3 where WEBSITE like '%baidu%' and S_SEQ=1 and C_SEQ=1").collect.foreach(println)
正常的情况下，比Hive SQL快20倍。

-------------------------------------------------------------------
参考官方doc练习,启动Spark集群
val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val df = sqlContext.read.json("examples/src/main/resources/people.json")
// Displays the content of the DataFrame to stdout
df.show()
df.printSchema()
df.select("name").show()
df.select(df("name"), df("age") + 1).show()
df.filter(df("age") > 21).show()
df.groupBy("age").count().show()

sqlContext
hiveContext
说明可以创建很多hiveContext，hiveContext连的是数据仓库，作为程序本身，
例如Spark中的Job可以并行运行，所以可以有多个hiveContext实例。查询数据级别有多个实例很正常。
val df = sqlContext.read.josn(“/library/examples/src/main/resources/people.json”)
把数据读取进来，知道数据的基本信息，json数据是key-value级别。


使用Java编写DataFrame   See:main/java/spark_sql/DataFrameOps.java
使用Scala编写DataFrame  See:main/scala/spark_sql/DataFrameOpsScala.scala

maven 打包参考http://www.cnblogs.com/yjmyzz/p/4694219.html
导包后,使用一下命令提交:
spark-submit --class spark_sql.DataFrameOpsScala --master spark://hadoop:7077 /home/hadoop/Document/export/xx.jar
业界为了统一，提交的过程，加上一个 --file,指定额外的配置文件,不用也没问题,主要是没用Hive的功能。
--files $SPARK_HOME/conf/hive-site.xml
或写入到一个shell脚本中:

/usr/local/spark/spark-1.6.0-bin-hadoop2.6/bin/spark-submit
--class com.dt.spark.sql.DataFrameOps
--files $HIVE_HOME/conf/hive-site.xml
--driver-class-path $SPARK_HOME/lib/mysql-connector-java-5.1.35-bin.jar
--master spark://Master:7077 /home/hadoop/Document/export/WordCount.jar

这样可以配置Hive的不同数据来源，如果不配置，就找Hive默认的配置信息。
数据仓库可能有不同的数据仓库。这么写是有道理的。Spark SQL当然可以用HQL。
如果Json数据嵌套比较复杂，先做一下ETL。






