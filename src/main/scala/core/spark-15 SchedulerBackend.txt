TaskScheduler与SchedulerBackend
FIFO与FAIR两种调度模式彻底解密
Task数据本地性资源分配源码实现

一:通过Spark-shell运行程序来观察TaskScheduler内幕
	1.当我们启动Spark-shell本身的时候命令终端反馈回来的主要是ClientEndpoint和SparkDeploySchedulerBackend,
		这是因为此时还没有任何Job的触发,这是启动Application本身而已,
		所以主要就是实例化SparkContext并注册当前的应用程序给Master且从集群中获得ExecutorBackend计算资源;
	2.DAGScheduler划分好Stage后会通过TaskSchedulerImpl中TaskSetManager来管理当前要运行的Stage中的所有任务TaskSet,
	TaskSetManager会根据locality aware 来为Task分配计算资源,监控Task的执行状态(例如重试,慢任务进行推测式执行等):参考补充1

二:TaskScheduler与SchedulerBackend
	1.总体的底层任务调度过程如下:
		a)TaskSchedulerImpl.submitTasks:主要的作用是将TaskSet加入到TaskManager中进行管理;
		b)SchedulableBuilder.addTaskSetManager:
			SchedulableBuilder会确定TaskSetManager的调度顺序,
			然后按照TaskSetManager的locality aware来确定每个Task具体运行在哪个ExecutorBackend中;

			schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties),参考补充2

		c)CoarseGrainedSchedulerBackend.reviveOffers:
			给DriverEndpoint发送ReviveOffers,ReviveOffers本身是一个空的case object对象,
			只是起到触发底层资源调度的作用,在有Task提交或者计算资源变动的时候会发送ReviveOffers这个消息作为触发器;

			backend.reviveOffers()
			-->CoarseGrainedSchedulerBackend.reviveOffers():
				driverEndpoint.send(ReviveOffers)
			-->case object ReviveOffers extends CoarseGrainedClusterMessage

		d)在DriverEndpoint接受ReviveOffers消息并路由到makeOffers具体的方法中,在makeOffers方法中:
		首先准备好所有可以用于计算的workOffers(代表了所有可用ExecutorBackend中可以使用的Cores等信息)

			class DriverEndpoint.receive{
				case ReviveOffers =>
				makeOffers()
				-->
			private def makeOffers() {
			  // Filter out executors under killing
			  val activeExecutors = executorDataMap.filterKeys(executorIsAlive)
			  val workOffers = activeExecutors.map { case (id, executorData) =>
				new WorkerOffer(id, executorData.executorHost, executorData.freeCores)
			  }.toSeq
			  launchTasks(scheduler.resourceOffers(workOffers))
			}

		e)TaskSchedulerImpl.resourceOffers:
		为每一个Task具体分配计算资源,输入是ExecutorBackend及其上可以用的Cores,输出TaskDescription的二维数组,
		在其中确定了每个Task具体运行在哪个ExecutorBackend.其中TaskScheduler参考补充3,TaskDescription参考补充4;

		 /**
		   * Called by cluster manager to offer resources on slaves. We respond by asking our active task
		   * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so
		   * that tasks are balanced across the cluster.
		   */
		  def resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {

		resourceOffers到底是如何确定Task具体运行在哪个ExecutorBackend红色那个的呢?
		算法的具体实现如下:
		i.通过Random.shuffle方法重新洗牌所有的计算资源以寻求计算的负载均衡;
			val shuffledOffers = Random.shuffle(offers)
		ii:根据每个ExecutorBackend的cores的个数声明类型为TaskDescription的ArrayBuffer数组;
			 val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores))
		iii:如果有新的ExecutorBackend分配给我们的Job,此时会调用executorAdded来获得最新的完成的计算计算资源,
			 for (taskSet <- sortedTaskSets) {
				 if (newExecAvail) {
				taskSet.executorAdded()
				}
		iv:通过下述代码最求最高级别的优先级本地性:参考补充5

    		//二维循环:
			for (taskSet <- sortedTaskSets; maxLocality <- taskSet.myLocalityLevels) {
			  do {
				launchedTask = resourceOfferSingleTaskSet(
					taskSet, maxLocality, shuffledOffers, availableCpus, tasks)
			  } while (launchedTask)
			}

		resourceOfferSingletaskSet参考:补充6
		v:通过调用TaskSetManager的resourceOffer最终去顶每个Task具体运行在那个ExecutorBackend的具体的LocalityLevel;

			for (task <- taskSet.resourceOffer(execId, host, maxLocality)) {
			-->

		f):通过launchTasks把任务发送给ExecutorBackend去执行;可联合参考补充8
	
	
		补充:1.Task默认的最大重试次数是4次;
		TaskSchedulerImpl类构造方法中:
		  def this(sc: SparkContext) = this(sc, sc.conf.getInt("spark.task.maxFailures", 4))
	  
		回到CoarseGrainedSchedulerBackend.DriverEndpoint. makeOffers()方法中:
			launchTasks(scheduler.resourceOffers(workOffers))
	
	2.Spark应用程序目前支持两种调度:FIFO,FAIR,
	可以通过spark-env.sh中spark-env.sh中spark.scheduler.mode进行具体的设置,
	默认情况下是FIFO的方式;

		trait SchedulableBuilder {
		private[spark] class FIFOSchedulableBuilder
		private[spark] class FairSchedulableBuilder}
		TaskSchedulerImpl类:
		private[spark] class TaskSchedulerImpl(
			  private val schedulingModeConf = conf.get("spark.scheduler.mode", "FIFO")

	3.TaskScheduler中要负责为Task分配计算资源:
	此时程序已经具备集群中的计算资源了根据计算本地性原则确定Task具体要运行在哪个ExecutorBackend中;
	
	4.TaskDescription中已经确定好了Task具体要运行在哪个ExecutorBackend上;

		/**
		 * Description of a task that gets passed onto executors to be executed, usually created by
		 * [[TaskSetManager.resourceOffer]].
		 */
		private[spark] class TaskDescription(
			val taskId: Long,
			val attemptNumber: Int,
			val executorId: String,
			val name: String,
			val index: Int,    // Index within this task's TaskSet
			_serializedTask: ByteBuffer)
		  extends Serializable {

	而确定Task具体运行在哪个ExecutorBackend上的算法是有TaskSetManager的resourceOffer方法决定
	
	5.数据本地性有限级别从高到底以此为:
	优先级高低排:PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY,
	其中NO_PREF是指机器本地性(一台机器可能包括多个node)

		object TaskLocality extends Enumeration {
	  // Process local is expected to be used ONLY within TaskSetManager for now.
	  val PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY = Value

	6.每个Task默认采用一个线程进行计算的
	 if (availableCpus(i) >= CPUS_PER_TASK) {
	  其中:val CPUS_PER_TASK = conf.getInt("spark.task.cpus", 1)

	7.DAGScheudler是从数据层面考虑preferedLocation的,而TaskScheduler是从具体计算Task角度考虑计算本地性;
	8.Task进行广播时候的AkkFrameSize大小是128MB;

		launchTasks(scheduler.resourceOffers(workOffers)){
		  if (serializedTask.limit >= akkaFrameSize - AkkaUtils.reservedSizeBytes) {
			-->private val akkaFrameSize = AkkaUtils.maxFrameSizeBytes(conf)
				-->def maxFrameSizeBytes(conf: SparkConf): Int = {
				val frameSizeInMB = conf.getInt("spark.akka.frameSize", 128)

	如果任务大于128Mb-200kb的话,则Task会直接丢弃掉;
	如果小于128Mb-200kb的话会通过CoarseGrainedExecutorBackend去launchTask到具体ExecutorBackend上;

	 if (serializedTask.limit >= akkaFrameSize - AkkaUtils.reservedSizeBytes) {
	  taskSetMgr.abort(msg)
	 } else {
	  val executorData = executorDataMap(task.executorId)
          executorData.freeCores -= scheduler.CPUS_PER_TASK
          executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))
      }
      -->CoarseGrainedExecutorBackend.receive(){
		  	case LaunchTask(data) =>
		  if (executor == null) {
		    logError("Received LaunchTask command but executor was null")
		    System.exit(1)
		  } else {
		    val taskDesc = ser.deserialize[TaskDescription](data.value)
		    logInfo("Got assigned task " + taskDesc.taskId)
		    executor.launchTask(this, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,
		      taskDesc.name, taskDesc.serializedTask)
		  }

      
其他参考:
Spark 内置TaskScheduler级别的调度算法，分别是队列与公平，默认是队列方式。具体源码可参考SchedulableBuilder
准确来讲spark有两层公平，一层是公平调度，一层是公平算法，如此便可使用公平调度+公平算法 或 公平调度+队列算法
	公平调度方面，spark的多队列设置在standlong模式下则是较为鸡肋，原因在于没有动态选择队列执行任务的权利，
	同时公平调度仅仅是单应用下产物，故此设置多队列则也无法发挥其优势，使用配置文件设置则无法得到应用。
	如利用spark长生命周期管理的程序，不断提交任务，则可以于代码级进行控制
	（我是选择如此控制，效果也较为明显，但硬代码依然要编写一小部分），参考如下代码，每次仅仅一个固定的poolName。
    sc.setLocalProperty("spark.scheduler.pool", "系统1")
    终究使用原生态的spark的同学较多，故此会显得公平调度有些鸡肋，其实不然，
    相对FIFO模型的默认使用POOL的方式讲，
    其实FAIR已有对POOL调用做了部分的优化，可以同时执行数个独立stage。
	FIFO:
	    override def addTaskSetManager(manager: Schedulable, properties: Properties) {
	        rootPool.addSchedulable(manager)
	    }
	POOL：
		/* 添加Schedulable */
		override def addSchedulable(schedulable: Schedulable) {
			require(schedulable != null)
			schedulableQueue.add(schedulable)
			schedulableNameToSchedulable.put(schedulable.name, schedulable)
			schedulable.parent = this
		}
	FAIR:
	    /** 添加TaskSetManager,决定具体执行pool
         *  @param manager TaskSetManager
         *  @param properties sparkContext 内置配置文件 */
        overridedef addTaskSetManager(manager: Schedulable, properties: Properties) {
          var poolName = DEFAULT_POOL_NAME
          var parentPool = rootPool.getSchedulableByName(poolName)
          // TODO 没有properties，则直接添加，没有该名称pool，则重新创建
          if (properties != null) {
            poolName = properties.getProperty(FAIR_SCHEDULER_PROPERTIES,DEFAULT_POOL_NAME)
            parentPool = rootPool.getSchedulableByName(poolName)
            if (parentPool == null) {
              // we will create a new pool that user has configured in app
              // instead of being defined in xml file
              parentPool = newPool(poolName,DEFAULT_SCHEDULING_MODE,
                DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT)
              rootPool.addSchedulable(parentPool)
              logInfo("Created pool %s, schedulingMode:%s, minShare: %d, weight: %d".format(
                poolName, DEFAULT_SCHEDULING_MODE,DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT))
            }
          }
          parentPool.addSchedulable(manager)
          logInfo("Added task set " + manager.name + " tasks to pool " + poolName)
        }

	TaskSetManage（无调度）:
		override def addSchedulable(schedulable: Schedulable) {}
	关于外层调用：
		val sortedSchedulableQueue =
		      schedulableQueue.toSeq.sortWith(taskSetSchedulingAlgorithm.comparator)
	算法模型是由spark.scheduler.allocation.file决定，也就是我们可以使用FAIR调度+FIFO算法，FAIR调度+FAIR算法的组合选择，算法决定的是pools队列资源的争取算法
	至于算法方面，可参考代码（SchedulingAlgorithm）：
		FIFO算法则较为简单，仅仅使用TaskSet优先级判断，执行顺序。
	    private[spark]class FIFOSchedulingAlgorithm extendsSchedulingAlgorithm {
	      overridedef comparator(s1: Schedulable, s2: Schedulable): Boolean = {
	        val priority1 = s1.priority
	        val priority2 = s2.priority
	        var res = math.signum(priority1 - priority2)
	        if (res == 0) {
	          val stageId1 = s1.stageId
	          val stageId2 = s2.stageId
	          res = math.signum(stageId1 - stageId2)
	        }
	        if (res < 0) {
	          true
	        } else {
	          false
	        }
	      }
	    }

		FAIR算法则是正常公平机制算法，如使用过Hadoop的同学则不会陌生，两者原理思路,编写顺序都是一样，仅仅yarn使用内存作为衡量标准，spark则使用运行任务数作为衡量标准，同时scala代码表现更加简洁.
		See: hadoop的yarn中源码:FairShareComparator
	spark:
		See : FairSchedulingAlgorithm
		主要涉及的参数有：
		spark.scheduler.mode  决定使用pool级别的调用模型
		spark.scheduler.allocation.file 公平调度配置文件，可以决定pool级别的任务调度
		spark.scheduler.pool  执行应用进入哪个pool,配置文件配置无效，仅代码设置有效。









