Task执行原理流程图
Task执行内幕源码解密
Task执行结果在Driver上处理解密

Executor会通过TaskRunner在ThreadPool来运行具体的Task,
TaskRunner内部会做一些准备工作,例如反序列化Task,然后通过网络来获取需要的文件,Jar包
-->运行Thread的run方法,导致Task的runTask被调用来完成具体的业务逻辑处理
==>在Task的runTask内部会调用RDD的iterator()方法,
	该方法就是我们针对当前Task所对用的Partition进行计算的关键之所在,
	在具体的处理内容会迭代Partition的元素并交给我们自定义的function进行处理,
	runTask有两种实现:ShuffleMapTask和ResultTask

	-->ShuffleMapTask.runTask()在计算具体的Partition之后,
	实际上会通过ShuffleManager获得的ShuffleWriter把当前Task计算的结果根据具体的ShuffleManager实现来写入到具体的文件.
	操作完成后会把MapStatus发送给DAGScheduler-->(把MapStatus汇报给MapOutputTracker)
	-->Driver[DAGScheduler(MapOutputTracker]
	-->(MapOutputTracker会把ShuffleMapTask执行结果交给ResaultTask)
	-->ResaultTask:根据前面Stage的执行结果进行Shuffle产生整个Job最后的结果


一:Task执行结果处理及原理流程图和源码解密:
	1.当Driver中的(Standalone模式)CoarseGrainedSchedulerBackend给CoarseGrainedExecutorBackend发送LaunchTask后,
	CoarseGrainedExecutorBackend收到消息,首先会反序列化TaskDescription:

		CoarseGrainedExecutorBackend.receive(){
		case LaunchTask(data) =>
			val taskDesc = ser.deserialize[TaskDescription](data.value)

	2.Executor会通过launchTask来执行Task;

		executor.launchTask(this, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,
		  taskDesc.name, taskDesc.serializedTask)
	  }
		-->def launchTask(
		  context: ExecutorBackend,
		  taskId: Long,
		  attemptNumber: Int,
		  taskName: String,
		  serializedTask: ByteBuffer): Unit = {

	3.Executor会通过TaskRunner在ThreadPool来运行具体的Task,
	在TaskRunner的run方法TaskRuner中首先会通过调用statusUpdata给Driver发信息汇报自己点状态说明自己是RUNNING状态

		Executor.launchTask(){
		val tr = new TaskRunner(context, taskId = taskId, attemptNumber = attemptNumber, taskName,
		  serializedTask)
		-->class TaskRunner.run(){
			 execBackend.statusUpdate(taskId, TaskState.RUNNING, EMPTY_BYTE_BUFFER)

	4.TaskRunner内部会做一些准备工作:
	例如反序列化Task的依赖,然后是通过网络下载这些依赖

    	val (taskFiles, taskJars, taskBytes) = Task.deserializeWithDependencies(serializedTask)
    	updateDependencies(taskFiles, taskJars)

	5.然后是反序列化Task本身

		task = ser.deserialize[Task[Any]](taskBytes, Thread.currentThread.getContextClassLoader)

	6.调用反序列化后的Task.run方法来执行任务并获得执行结果

         val (value, accumUpdates) = try {
          val res = task.run(
            taskAttemptId = taskId,
            attemptNumber = attemptNumber,
            metricsSystem = env.metricsSystem)
          threwException = false
          res

	其中Task.run方法调用时候会导致Task的抽象方法runTask()调用;
	对于ShuffleMapTask,首先要对RDD以及其依赖关系进行反序列化:
	
		Task.run(){
			.....
			(runTask(context), context.collectAccumulators())
		}
		-->Task.runTask(context: TaskContext): T

	在Task的runTask内部会调用RDD的iterator()方法,
	该方法就是我们针对当前Task所定影的partition进行计算的关键之所在,
	在处理的内部会迭代Partition的元素并交给我们自定义的function进行处理!
	例如:

		ShuffleMapTask.runTask(){
		val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](
		.....
		writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])

	对于Shuffle,首先要对RDD以及其依赖关心进行反序列化,同上代码
	最终计算会调用RDD的compute方法:

		-->RDD.iterator(){
			computeOrReadCheckpoint(split, context)
			-->RDD.compute(split: Partition, context: TaskContext)
			-->TaskContextImpl类

	具体计算的时候有具体的RDD,例如MapPartitionRDD的compute方法:

			compute(split: Partition, context: TaskContext): Iterator[U] =
		f(context, split.index, firstParent[T].iterator(split, context))

    其中的f就是我们在当前的Stage中计算具体Partition的业务逻辑代码;
    
    对于ResaultTask.runTask():代码

    7.把执行结果序列:
    并根据大小判断不同的结果传回给Driver的方式

		val serializedResult: ByteBuffer = {...
		 if (maxResultSize > 0 && resultSize > maxResultSize) {...
		 }else if (resultSize >= akkaFrameSize - AkkaUtils.reservedSizeBytes) {...
		 }else {..
		 }
		 execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)

	8.CoarseGrainedExecutorBackend给DriverEndpoint发送StatusUpdate来传输执行结果,
	DriverEndpoint会把执行结果传递给TaskSchedulerImpl处理,
	然后交给TaskResultGetter内部通过线程去分别处理Task执行成功和失败时候的不同情况,
	然后告诉DAGScheduler任务处理结束的状况

		 -->CoarseGrainedExecutorBackend.statusUpdate(){
		 case Some(driverRef) => driverRef.send(msg)
		 -->CoarseGrainedSchedulerBackend.DriverEndpoint.receive(){
		  case StatusUpdate(executorId, taskId, state, data) =>
			scheduler.statusUpdate(taskId, state, data.value)
			-->TaskSchedulerImpl.statusUpdate()

	2.运行Thread的run方法,导致Task的runTask被调用来执行具体的业务逻辑处理
	
	
	补充说明:
	1.在执行具体的Task的业务逻辑前会进行三次反序列化:分别是
		a)TaskDescription的反序列化
		b)反序列化Task的依赖
		b)Task的反序列化
		c)RDD的反序列化
	2.在Spark1.6中AkkFrameSize是128MB,所以可以广播非常大的任务,而任务的执行结果可以最大达到1Gb;

		val serializedResult: ByteBuffer = {
			if (maxResultSize > 0 && resultSize > maxResultSize) {
			 // Limit of bytes for total size of results (default is 1GB)
			 -->private val maxResultSize = Utils.getMaxResultSize(conf)

其他参考:Task向Executor提交的源码解析
	http://blog.csdn.net/anzhsoft/article/details/40238111

---------------------------------------------------------------------------------------------------
一：Spark集群部署
二：Job提交解密
三：Job生成和接受
四：Task的运行
五：再论shuffle

１,从spark Runtime的角度讲来讲有５大核心对象:
Master,Worker,Executor,Driver,CoarseGraindExecutorbacked
２,Spark在做分布式集群系统的设计的时候，最大化功能的独立，
	模块化封装具体的独立的对象，强内聚低耦合
	（耦合性也称块间联系，指软件系统结构中各模块间相互联系紧密程度的一种度量。
		模块之间联系越紧密，其耦合性就越强，模块的独立性则越差。
		模块间耦合高低取决于模块间接口的复杂性、调用的方式及传递的信息。
	内聚性又称块内联系。指模块的功能强度的度量，即一个模块内部各个元素彼此结合的紧密程度的度量。
		若一个模块内各元素（语名之间、程序段之间）联系的越紧密，则它的内聚性就越高）
３,当Driver中的SparkContext初始化的时候会提交程序给Master，
	Master如果接受该程序在spark中运行的话，就会为当前程序分配AppID，同时分配计算资源，

	需要特备注意的是：
	Master是根据当前程序的配置信息来给集群中的Worker发指令来分配具体的计算资源。
	但是，Master发指令后并不关心具体的计算资源是否已经分配，
	转过来说，Master发出指令后就记录了分配的资源，以后客户端再次提交其他程序的话就不能使用该资源，

	其弊端是可能会导致其他要提交的程序无法分配到本来应该可以分配到的计算资源。
	最终优势在spark分布式系统功能弱耦合的基础上最快的运行系统
		（否则如果Master要等到计算资源最终分配成功后才通知Driver的话，会造成Driver的阻塞，不能够最大化的并行计算资源的利用率）
		（低耦合：不关心指令发送成功还是失败）
		（快是对Driver而言）

二　:Job提交过程源码解密

	１，一个非常重要的技巧通过在Spark-Shell中运行一个Job来了解Job提交的过程，然后再次用源码验证。
		这个过程：　
			sc.textFIle("src/main/resources/text.txt").flatMap(_.split(" "))
			.map(word=>(word,1)).reduceByKey(_+_)
			.saveAsTextFile("target/out/wc.out")

	２，在Spark中所有的Action都会触发一个至少一个Job，在上述代码中通过saveAsTextFile来触发Job的
	３.SparkContext在实例化的时候会构造SparkDeploySchedulerBackend(deploy:配置，部署），
		DAGScheduler,TaskShedulerImpl(Impl:接口），MapOutputTrackerMaster等对象：
		（1）SparkDeploySchedulerBackend负责集群计算资源的管理和调度。
		（2）DAGScheduler ： 负责高层调度（例如： Job中stage的划分，数据本地性等内容）
		（3）TaskSchedulerImpl : 负责具体stage内部的底层调度（例如： 每个Task的调度 ，Task容错等等）
		（4）MapOutputTrackerMaster： 负责shuffle中数据的输出和读取的管理。
		补充说明的是：
		Spark默认程序是排队的，Spark默认的情况下由于集群中一般都只有一个Application在运行，
		所以Master分配计算资源策略就没有那么明显啦
	4，TaskSchedulerImpl内部的调度：

三：Task的运行解密：
１，Task运行在Executor中，而Executor又是位于CoarseGrainedExecutorBackend中的,
	且CoarseGrainedExecutorBackend和Executor是一一对应的
２，单CoarseGrainedExecutorBackend接受到TaskSetManager发过来的LaunchTask的消息后,
	会反序列化TaskDescription，然后使用CoarseGrainedExecutorBackend中唯一的Executor来执行任务

		case LaunchTask(data) =>
		if (executor == null) {
		logError(“Received LaunchTask command but executor was null”)
		System.exit(1)
		} else {
		val taskDesc = ser.deserializeTaskDescription
		logInfo(“Got assigned task ” + taskDesc.taskId)
		executor.launchTask(this, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,
		taskDesc.name, taskDesc.serializedTask)
		}
	发消息要么是case Class或者case object（是唯一的）每次生成类的事例
