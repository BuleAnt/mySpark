Hash Shuffle彻底解密
Shuffle Pluggable解密
Sorted Shuffle解密
Shuffle性能优化

一,到底什么是Shuffle?
	Shuffle中文翻译为"洗牌",需要Shuffle的关键性原因是某种具有共同特性的数据需要最终汇聚到一个计算节点上进行计算

二,Shuffle可能面临的问题:运行Tesk的时候才会产生Shuffle(Shuffle已经融合在Spark 的算子中了)
	1.数据量非常大
	2.数据如何分类,即如何partition;Hash,Sort,钨丝计算;
	3.负载均衡(数据倾斜);
	4.网络传输效率;需要在压缩和解压缩之间做出权衡,序列化和反序列化也是要考虑的问题;
	说明:具体的Task进行计算的时候尽一切最大可能使得数据具备Process Locality的特性;退而求其次是增加数据分片,减少每个Task处理的数据量.
	
三,Hash Shuffle
	1.Key不能是Array
	2.Hash Shuffle不需要排序,此时从理论上讲就节省了Hadoop MapReduce中进行SHuffle需要排序时候的时间浪费,因为实际生产环境下有大量的不需要排序的Shuffle类型;
	思考:不需要排序的Hash Shuffle是否一定比需要排序的Sorted Shuffle速度更快?
	不一定,如果数据规模比较小的情况下,Hash Shuffle会比Sorted Shuffle速度快很多!但是如果数据量大,此时Sorted Shuffle一般会比Hash Shuffle快(很多)
	3.每个ShuffleMapTask会根据Key的哈希值计算出当前的Key需要的Partition,然后把决定后的结果写入到单独的文件中,此时会导致每个Task产生R(指下一个Stage的并行度)个文件,如果当前的Stage中有M个ShuffleMapTesk,则一共会产生M*R个文件!!!
	注意:Shuffle操作绝大多数情况下都要通过网络,如果Mapper和Reducer在同一台机器上,此时只需要读取本地磁盘即可.
	
源码:
class HashPartitioner(partitions: Int) extends Partitioner {
  require(partitions >= 0, s"Number of partitions ($partitions) cannot be negative.")

  def numPartitions: Int = partitions

  def getPartition(key: Any): Int = key match {
    case null => 0
    case _ => Utils.nonNegativeMod(key.hashCode, numPartitions)
  }

  override def equals(other: Any): Boolean = other match {
    case h: HashPartitioner =>
      h.numPartitions == numPartitions
    case _ =>
      false
  }
  继承的Partitioner如下:
  object Partitioner {
  /**
   * Choose a partitioner to use for a cogroup-like operation between a number of RDDs.
   *
   * If any of the RDDs already has a partitioner, choose that one.
   *
   * Otherwise, we use a default HashPartitioner. For the number of partitions, if
   * spark.default.parallelism is set, then we'll use the value from SparkContext
   * defaultParallelism, otherwise we'll use the max number of upstream partitions.
   *
   * Unless spark.default.parallelism is set, the number of partitions will be the
   * same as the number of partitions in the largest upstream RDD, as this should
   * be least likely to cause out-of-memory errors.
   *
   * We use two method parameters (rdd, others) to enforce callers passing at least 1 RDD.
   */
  def defaultPartitioner(rdd: RDD[_], others: RDD[_]*): Partitioner = {
    val bySize = (Seq(rdd) ++ others).sortBy(_.partitions.size).reverse
    for (r <- bySize if r.partitioner.isDefined && r.partitioner.get.numPartitions > 0) {
      return r.partitioner.get
    }
    if (rdd.context.conf.contains("spark.default.parallelism")) {
      new HashPartitioner(rdd.context.defaultParallelism)
    } else {
      new HashPartitioner(bySize.head.partitions.size)
    }
  }

	Hash Shuffle的两大死穴:第一,Shuffle前会产生海量的小文件于磁盘之上,此时会产生大量的耗时的低效的IO操作.第二,内存不够用!!!由于内存中需要保存海量的文件操作句柄和临时缓存信息,如果数据处理规模比较庞大的化,内存不可承受,出现OOM等问题
	为了改善上述的问题(同时打开过多文件导致Writer Handler内存使用过大,以及产生过多文件导致大量的随机读写,带来效率极为底下的磁盘IO操作),Spark后来推出了Consalidate机制,来把小文件合并,此时Shuffle时文件产生数量为cores*R,对于ShuffleMapTesk的数量明显多余同事可用的并行Cores的数量的情况下,Shuffle产生的文件大幅度减少,会极大降低OOM的可能
	
	为此Spark退出了Shuffle Pluggable开放框架,方便系统升级的时候定制Shuffle功能模块,也方便第三方系统改造人员根据实际的业务场景来开放具体最佳的Shuffle模块;和兴接口ShuffleManager,具体默认实现有HashShuffleManager, SortShuffleManager等,Spark 1.6.0中具体的配置:(下一段)
	
/**
 * Pluggable interface for shuffle systems. A ShuffleManager is created in SparkEnv on the driver
 * and on each executor, based on the spark.shuffle.manager setting. The driver registers shuffles
 * with it, and executors (or tasks running locally in the driver) can ask to read and write data.
 *
 * NOTE: this will be instantiated by SparkEnv so its constructor can take a SparkConf and
 * boolean isDriver as parameters.
 */
private[spark] trait ShuffleManager {
  /**
   * Register a shuffle with the manager and obtain a handle for it to pass to tasks.
   */
  def registerShuffle[K, V, C](
      shuffleId: Int,
      numMaps: Int,
      dependency: ShuffleDependency[K, V, C]): ShuffleHandle

  /** Get a writer for a given partition. Called on executors by map tasks. */
  def getWriter[K, V](handle: ShuffleHandle, mapId: Int, context: TaskContext): ShuffleWriter[K, V]

  /**
   * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive).
   * Called on executors by reduce tasks.
   */
  def getReader[K, C](
      handle: ShuffleHandle,
      startPartition: Int,
      endPartition: Int,
      context: TaskContext): ShuffleReader[K, C]

  /**
    * Remove a shuffle's metadata from the ShuffleManager.
    * @return true if the metadata removed successfully, otherwise false.
    */
  def unregisterShuffle(shuffleId: Int): Boolean

  /**
   * Return a resolver capable of retrieving shuffle block data based on block coordinates.
   */
  def shuffleBlockResolver: ShuffleBlockResolver

  /** Shut down this ShuffleManager. */
  def stop(): Unit
}
具体默认实现有HashShuffleManager, SortShuffleManager等,Spark 1.6.0中具体的配置:(有三种类型)
SParkEnv中:
 val shortShuffleMgrNames = Map(
      "hash" -> "org.apache.spark.shuffle.hash.HashShuffleManager",
      "sort" -> "org.apache.spark.shuffle.sort.SortShuffleManager",
      //钨丝计划(解决GC问题)
      "tungsten-sort" -> "org.apache.spark.shuffle.sort.SortShuffleManager")
可配置在spark-default.conf
默认为sort方式,那sort如何解决内存和小文件问题:
hadoop mapreduce由于数据规模特别大,进行shuffle默认sort排序,partition使用索引
spark sort同样使用index,不需要排序的使用hash分区,结束的时候进行归并排序


为什么使用Sort-Based Shuffle
Sort-Based Shuffle实战
Sort-Based Shuffle内幕
Sort_Based Shuffle的不足

一:为什么使用Sort-Based Shuffle
	1.shuffle一般包含两个阶段任务:第一部分,产生Shuffle数据的阶段(Map阶段,额外补充,需要实现ShuffleManager中getWriter来写数据,(数据可以BlockManager写到Memory,Disk,Tachyon等,例如想非常快的Shuffle,此时可以考虑吧数据写在内存中,但是内存不稳定,建议采用MEMORY_AND_DISK方式));第二部分,使用Shuffle数据的阶段(Redcue阶段,额外补充,需要事项ShuffleManager的getReader,Reader会向Driver去获取上一个Stage产生的Shuffle数据)
	2.Spark的Job会被划分成很多Stage,如果只有一个Stage,则这个Job就相当于只有一个Mapper阶段,当然不会产生Shuffle,适合于简单的ETL;
	如果不止一个Stage,则最后一个Stage就是最终的Reducer,最左侧的第一个Stage就仅仅是整个Job的Mapper,中间所有的任意Stage是其父Stage的Redcuer且是其子Stage的Mapper
	3.Spark Shuffle在最开始的时候只支持Hash-based Shuffle:默认Mapper阶段回味Reducer阶段的每一个Task单独创建一个文件来保证该Task中要使用的数据,
	但是在一些情况下(例如数据量非常大的情况)会造成大数据(大量文件:M*R,M表示Mapper中所有的并行任务,R表示Reducer中所有的并行任务数量)的随机磁盘I/O操作,且会性能大量的Mempry消耗(极易造成OOM),这是致命的问题,因为第一不能够处理大规模的数据,第二Spark不能够运行在大规模的分布式集群上!
	后来的改善方式是加入Shuffle Consolidate机制来将Shuffle时候产生的文件数量减少到C*R个(c表示Mapper端同事能够使用Cores的数量,R表示Reducer中所有并行任务数量),但是此时如果Reducer端的并行数据分片过多的话则C*R可能依旧过大,此时依旧没有逃脱文件打开过多的厄运!!!

	Spark在引入Sort-based Shuffle(Spark1.1版本以前)以前比较适用于中小型规模的大数据处理!
	4.为了让spark在更大规模的集群上更高性能处理更大规模的数据,于是就引入了Sort-based Shuffle(1.1版本开始正式的实现)

	从此以后(Spark1.1版本后),Spark可生人与任意规模(包含PB级别以及PB以上的级别)的数据处理,尤其是随着钨丝计划的引入和优化,把Spark更快的在更大规模的集群处理更海量的数据的能力推向了一个新的巅峰
(参考:	Hadoop的Map阶段,map中环形缓冲区(数组:数据+索引),内存消耗达到阈值后,spill->)
源代码:
	
	5.spark1.6版本支持至少三种类型Shuffle:
	源码:
SParkEnv中:
 val shortShuffleMgrNames = Map(
      "hash" -> "org.apache.spark.shuffle.hash.HashShuffleManager",
      "sort" -> "org.apache.spark.shuffle.sort.SortShuffleManager",
      //钨丝计划(解决GC问题)
      "tungsten-sort" -> "org.apache.spark.shuffle.sort.SortShuffleManager")
	Spark重要进步:
	实现ShuffleManager接口,可以更具自己的业务实际需要最优化的使用自定义的Shuffle实现
	
	6.Spark1.6默认采用的就是Sort-based Shuffle的方式;
	SParkEnv中:
	 val shortShuffleMgrNames = Map(
      "hash" -> "org.apache.spark.shuffle.hash.HashShuffleManager",
      "sort" -> "org.apache.spark.shuffle.sort.SortShuffleManager",
      "tungsten-sort" -> "org.apache.spark.shuffle.sort.SortShuffleManager")
    val shuffleMgrName = conf.get("spark.shuffle.manager", "sort")
    上述的源码说明,你可以在Spark的配置文件中配置Spark框架运行时要使用的具体的ShuffleManager的实现
    修改conf/spark-default.conf加入如下内容:
    spark.shuffle.manager SORT
    Sort-based shuffle不会为每个Reducer中的Task生成一个单独的文件,相反,Sort-based Shuffle会把Mapper中每个ShuffleMapTask所有的输出数据Data只写到一个文件中,因为每个ShuffleMapTask的数据会被分类,所以Sort-based Shuffle使用了index文件存储具体ShuffleMapTask输出数据在同一个Data文件中是如何分类的信息!!!所以说基于Sort-base的Shuffle会在Mapper中的每一个ShuffleMapTask中产生两个文件:Data文件和Index文件,其中Data文件是存储当前Task的Shuffle输出的,Index存储了Data文件中数据通过Partitioner的分类信息,此时洗衣歌阶段Stage中Task就是根据这个Index文件获取自己索要抓取的上一个Stage ShuffleMapTask产生的数据的
    
	Sort-based Shuffle产生的文件数量正确的答案是2M(M表示Mapper阶段中并行的Partition的总数量,其实就是Mapper端Task的总数量)个Shuffle临时文件
	
	回顾真个Shuffle的历史,Shuffle产生的临时文件的数量的变化依次是:
	Basic Hash Shuffle:M*R
	Consalidate 方式的Hash Shuffle:C*R
	Sort-based Shuffle:2*M
	
二:在集群中动手实战SortBaseShuffle:
	在Sort-based Shuffle中Reducer是如何获取自己需要数据的呢?
	具体而言,Reducer首先找Dirver去获取父Stage中每个ShffleMapTask输出的位置信息,根据位置信息获取index文件,解析index文件,从解析的index文件中获取Data文件中数据自己的那部分内容
	
三:默认Sort-based Shuffle 的几个缺陷:
	1.如果Mapper中Task的数量过大,依旧会产生很多小文件;此时在Shuffle传递数据的过程中到Reducer端,reduce会需要同时大量的记录来进行反序列化,导致大量的内存消耗和GC的据他负担,造成系统宦帆甚至崩溃!
	2.如果需要在分片内也进行配需的话,此时需要进行Mapper端和Reducer端的两次的排序!!!
	3.
