Scheduler方法解析:
为Drvier分配资源
为Application分配资源
两种不同的资源分配方式彻底解密 
Spark资源分配的思想

一:任务调度与资源调度的区别
	1.任务调度是通过DAGScheduler,TaskScheduler,SchedulerBackend等进行的作业调度
	2.资源调度是指应用程序获得资源
	3.任务调度是在资源调度的基础上进行,没有资源调度那么任务调度就成为了无源之水,无本之木
	
二:资源调度内幕解密
	1.因为Master负责资源管理和调度,所有资源调度的方法schedule位于Master.scala这个类中,当注册程序或资源发生改变的时候都会导致schedule的调用,例如注册程序的时候:

def receive: PartialFunction[Any, Unit] = {
    case RegisterApplication(description, driver) => {
      // TODO Prevent repeated registrations from some driver
      if (state == RecoveryState.STANDBY) {
        // ignore, don't send response
      } else {
        logInfo("Registering app " + description.name)
        val app = createApplication(description, driver)
        registerApplication(app)
        logInfo("Registered app " + description.name + " with ID " + app.id)
        persistenceEngine.addApplication(app)
        driver.send(RegisteredApplication(app.id, self))
        schedule()
      }
    }
	2.Schedule调用的时机:每次有新的应用程序提交或者集群资源状况发生改变的时候(包括Executor增加或者减少,Worker增加或者减少等);
	  /**
   * Schedule the currently available resources among waiting apps. This method will be called
   * every time a new app joins or resource availability changes.
   */
  private def schedule(): Unit = {

	3.当前Master必须是Alive的方式采用进行资源的调度;如果不是ALIVE的状态会直接返回,也就是StandbyMaster不会进行Application资源的调用
    if (state != RecoveryState.ALIVE) { return }
    
    4.使用Random.shuffle把Master中保留的集群中所有Worker的信息随机打乱;
    // Drivers take strict precedence over executors
    val shuffledWorkers = Random.shuffle(workers) // Randomization helps balance drivers
    //其算法内部是循环随机交换所有Worker在Master缓存数据节后中的位置
	5.接下来要判断所有Worker中那些是ALIVE级别的Worker,ALIVE才能够参与资源的分配工作
    for (worker <- shuffledWorkers if worker.state == WorkerState.ALIVE) {
	6.当SparkSubmit指定Driver在Cluster模式的情况下,此时Driver会加入waitingDrivers等待列表中,在每个DriverInfo的DriverDescription中有要启动Dirver时候对Worker的内存以及Cores是要求等内容
DriverDescription源码:
private[deploy] case class DriverDescription(
    jarUrl: String,
    mem: Int,
    cores: Int,
    supervise: Boolean,
    command: Command)
    在符合资源要求的情况下然后采用随机打乱后的一个Worker来启动Driver;
      for (driver <- waitingDrivers) {
        if (worker.memoryFree >= driver.desc.mem && worker.coresFree >= driver.desc.cores) {
	Master发指令给Worker,让远程的Worker启动Driver
          launchDriver(worker, driver)
链接launchDriver源码:
  private def launchDriver(worker: WorkerInfo, driver: DriverInfo) {
    logInfo("Launching driver " + driver.id + " on worker " + worker.id)
    worker.addDriver(driver)
    driver.worker = Some(worker)
    worker.endpoint.send(LaunchDriver(driver.id, driver.desc))
    driver.state = DriverState.RUNNING
  }
    7.先启动Driver才会发生后续的一切的资源调度
          waitingDrivers -= driver
        }
      }
    }
    startExecutorsOnWorkers()
  }
	8.spark默认为应用程序启动Executor的方式是FIFO的方式,也就是说所有提交的应用程序都是放在调度的等待队列中的,先进先出,只有满足了前面应用程序的资源分配的基础上才能够满足下一个应用程序资源的分配;
private def startExecutorsOnWorkers(): Unit = {
    // Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app
    // in the queue, then the second app, etc.
	9.为应用程序具体分配Executor之前要判断应用程序是否还需要分配Core,如果不需要则不会为应用程序分配Executor;
    for (app <- waitingApps if app.coresLeft > 0) {
      val coresPerExecutor: Option[Int] = app.desc.coresPerExecutor
      // Filter out workers that don't have enough resources to launch an executor
	10.具体分配Executor之前要求Worker必须是ALIVE的状态,且必须满足Application对每个Executor的内存和Cores的要求,并且在此基础上进行排序;
      //计算资源有大到小的usableWorkers数据结构:
      val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE)
        .filter(worker => worker.memoryFree >= app.desc.memoryPerExecutorMB &&
          worker.coresFree >= coresPerExecutor.getOrElse(1))
        .sortBy(_.coresFree).reverse
        //在FIFO的情况下默认是spreadOutApps来让应用程序尽可能多的运行在所有的Node上.可以链接spreadOutApps类
      val assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)
      11.为应用程序分配Executors有两种方式,第一种方式是尽可能在集群的所有Worker上分配Executor,这种方式往往会带来潜在的更好的数据本地性
      scheduleExecutorsOnWorkers源码:
  /**
   * Schedule executors to be launched on the workers.
   * Returns an array containing number of cores assigned to each worker.
   *
   * There are two modes of launching executors. The first attempts to spread out an application's
   * executors on as many workers as possible, while the second does the opposite (i.e. launch them
   * on as few workers as possible). The former is usually better for data locality purposes and is
   * the default.
   *
   * The number of cores assigned to each executor is configurable. When this is explicitly set,
   * multiple executors from the same application may be launched on the same worker if the worker
   * has enough cores and memory. Otherwise, each executor grabs all the cores available on the
   * worker by default, in which case only one executor may be launched on each worker.
   *
   * It is important to allocate coresPerExecutor on each worker at a time (instead of 1 core
   * at a time). Consider the following example: cluster has 4 workers with 16 cores each.
   * User requests 3 executors (spark.cores.max = 48, spark.executor.cores = 16). If 1 core is
   * allocated at a time, 12 cores from each worker would be assigned to each executor.
   * Since 12 < 16, no executors would launch [SPARK-8881].
   */
  private def scheduleExecutorsOnWorkers(
      app: ApplicationInfo,
      usableWorkers: Array[WorkerInfo],
      spreadOutApps: Boolean): Array[Int] = {
    val coresPerExecutor = app.desc.coresPerExecutor
    val minCoresPerExecutor = coresPerExecutor.getOrElse(1)
    val oneExecutorPerWorker = coresPerExecutor.isEmpty
    val memoryPerExecutor = app.desc.memoryPerExecutorMB
    val numUsable = usableWorkers.length
    val assignedCores = new Array[Int](numUsable) // Number of cores to give to each worker
    val assignedExecutors = new Array[Int](numUsable) // Number of new executors on each worker
	12:具体在集群上分配Cores的时候会尽可能的满足我们的要求:
    var coresToAssign = math.min(app.coresLeft, usableWorkers.map(_.coresFree).sum)
    ....
	13.如果是每个Worker下面只能够为当前的应用程序分配一个Executor的话,每次是分配一个Core!
		// If we are launching one executor per worker, then every iteration assigns 1 core
          // to the executor. Otherwise, every iteration assigns cores to a new executor.
          if (oneExecutorPerWorker) {
            assignedExecutors(pos) = 1
          } else {
            assignedExecutors(pos) += 1
          }
	14.回到startExecutorsOnWorkers.
      // Now that we've decided how many cores to allocate on each worker, let's allocate them
      for (pos <- 0 until usableWorkers.length if assignedCores(pos) > 0) {
        allocateWorkerResourceToExecutors(
          app, assignedCores(pos), coresPerExecutor, usableWorkers(pos))
      }
    }
  }
==>def startExecutorsOnWorkers(): Unit = {
	for (i <- 1 to numExecutors) {
		  val exec = app.addExecutor(worker, coresToAssign)
		  launchExecutor(worker, exec)
		  app.state = ApplicationState.RUNNING
		}
	==>
	def launchExecutor(worker: WorkerInfo, exec: ExecutorDesc): Unit = {
	 logInfo("Launching executor " + exec.fullId + " on worker " + worker.id)
    worker.addExecutor(exec)
	14.准备具体要为当前应用程序分配的Executor信息后,Master要通过远程通信发送send指令给Worker,来启动ExecutorBackend进程
    worker.endpoint.send(LaunchExecutor(masterUrl,
      exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory))
	15.紧接着给我们应用程序的Driver发送一个Executor信息:
    exec.application.driver.send(
      ExecutorAdded(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory))
  }
  

