Job Stage划分算法解密
Task最佳位置算法实现解密

一:Job Stage划分算法解密
	1.Spark Application中可以因为不同的Action触发众多Job,也就是说一个Application可以有很多的Job,而每个Job是由一个或者多个Stage构成的,后面的Stage依赖于前面的Stage,也就是说只有前面依赖的Stage计算完毕后,后面的Stage才能够运行
	2.Stage划分的依据就是宽依赖,什么时候产生宽依赖呢?例如reduceByKey, groupByKey等
	3.有Action例如collect导致了SparkContext.runJob,最终导致了DAGScheduler众多submitJob执行,其核心是通过发送一个case class JobSubmitted对象给eventProcessLoop,其中JobSubmitted源码如下:
 def submitJob[T, U](..){
 	...
    eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter,
      SerializationUtils.clone(properties)))
JobSubmitted源码:
private[scheduler] case class JobSubmitted(
    jobId: Int,
    finalRDD: RDD[_],
    func: (TaskContext, Iterator[_]) => _,
    partitions: Array[Int],
    callSite: CallSite,
    listener: JobListener,
    properties: Properties = null)
  extends DAGSchedulerEvent
  
	//eventProcessLoop是DAGSchedulerEventProcessLoop的实例,而DAGSchedulerEventProcessLoop是EventLoop的子类(EventLoop有一个线程 private val eventThread = new Thread(name)如下),具体实现EvenLoop的onReceive方法,onReceive过来回调doOnReceive
	 private val eventThread = new Thread(name) {
    setDaemon(true)
    override def run(): Unit = {
        while (!stopped.get) {
          val event = eventQueue.take()
            onReceive(event)
	4.在doOnReceive中通过模式匹配的方式把执行路由到
DAGSchedulerEventProcessLoop类中:
	private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
	
	5.在hadleJobSubmitted中首先创建finalStage,创建finalStage时候会建立父Stage的依赖关系链条;
	 finalStage = newResultStage(finalRDD, func, partitions, jobId, callSite)

	补充说明:所谓的missing就是说要进行当前的计算了例如:
	 /** Returns the sequence of partition ids that are missing (i.e. needs to be computed). */
  def findMissingPartitions(): Seq[Int]
}
二:Task任务本地性算法实现:
 private def submitStage(stage: Stage) {
	if (missing.isEmpty) {
	 submitMissingTasks(stage, jobId.get)
	 ...
	 --> private def submitMissingTasks(stage: Stage, jobId: Int) {
	1.在submitMissingTasks中会通过调用一下代码来获得任务的本地性;
	 val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {
      stage match {
        case s: ShuffleMapStage =>
          partitionsToCompute.map { id => (id, getPreferredLocs(stage.rdd, id))}.toMap
        case s: ResultStage =>
          val job = s.activeJob.get
          partitionsToCompute.map { id =>
            val p = s.partitions(id)
            (id, getPreferredLocs(stage.rdd, p))
          }.toMap
	
其中:
	val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()
	
	2.具体一个Parition中的数据本地性的算法实现为下属代码中:
  private[spark]
  def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = {
    getPreferredLocsInternal(rdd, partition, new HashSet)
  }
  /**
   * Recursive implementation for getPreferredLocs.
   *
   * This method is thread-safe because it only accesses DAGScheduler state through thread-safe
   * methods (getCacheLocs()); please be careful when modifying this method, because any new
   * DAGScheduler state accessed by it may require additional synchronization.
   */
   private def getPreferredLocsInternal(
   在具体算法实现的时候首先查询DAGScheduler的内存数据结构中是否存在当前Parition的数据本地行的信息,如果有的话直接返回,如果没有的话首先会调用rdd.getPreferedLocations
    val cached = getCacheLocs(rdd)(partition)
    if (cached.nonEmpty) {
      return cached
    }
    // If the RDD has some placement preferences (as is the case for input RDDs), get those
    val rddPrefs = rdd.preferredLocations(rdd.partitions(partition)).toList
    if (rddPrefs.nonEmpty) {
      return rddPrefs.map(TaskLocation(_))
    }
   例如想让Spark运行在Hbase上或者一种现在还没有直接支持的数据库上面,此时开发者需要自定义RDD,为了保证Task计算的数据本地性,最为关键的方式就是必须实现RDD的getPreferedLocations;
   
   3.DAGSCheduler计算数据本地性的时候,巧妙的借助了RDD自身的getPreferedLocations中的数据,最大化的优化效率,以为getPreferedLocations中表明了每个Parititon的数据本地性,虽然当前Partition可能被persist或者checkpoint,但是persist或者checkpoint默认情况下肯定是和getPreferedLocations中的Partition的数据本地性是一致的,所以这就极大的简化Task数据本地性算法的实现和效率的优化;
	
其他参考:Stage划分及提交源码分析
	http://blog.csdn.net/anzhsoft/article/details/39859463
