val sc = new SparkContext(conf)
SparkContext初始化的过程主要的核心:
1,依据SparkContext的构造方法中国你的参数SparkConf创建一个SparkEnv
2,初始化,Spark UI,以便Spark Application在运行时,方便用户监控,默认端口为4040
3,创建和启动Scheduler
	创建一个TaskScheduler,SchedulerBackend
akka:心跳的接收器
	创建DAGScheduler
	启动TaskScheduler,DAGScheduler
4,启动Executor

standalone模式讲解:Spark架构
Application = driver+executor
SparkApp运行不依赖于Cluster Master,粗粒度,可拔插
Driver和Executor两个部分:
Driver:运行程序时候,具有main方法并创建sparkcontext实例,整个程序运行调度的核心,sparkcontext创建时候要有:SparkConf,SparkEnv,DAGScheduler,TaskScheduler,SchedulerBackend等
Dirver部分的代码:SparkConf+SparkContext

Executor:运行在worker节点,执行任务的组件,用于启动线程池运行任务,每个Application拥有独立的一组Executors(一个进程里的处理对象,为当前应用程序开启一个线程,通过线程池并发执行和线程复用,通过线程处理具体的Task),默认一个worker开启一个Executor

Worker管理当前Node的资源管理,并接受Master的指令来分配具体的计算资源Executor(在新的进程中分配),Worker向Master发送心跳内容只有WorkerID,不会报告资源使用情况等信息,Master在APP初始化时候已经分配了资源,后来不需要具体使用情况.
ExecutorRunner:Proxy,管理新分配的进程,远程创建Executor进程
Job:包含了一系列的Task的并行计算,由RDD的Action触发,一个Applicaton中可以有多个Job,一般一个Action可以对应一个Job,特殊情况下:Checkpoint,range
Stage内部计算逻辑完全基于内存计算,只是计算的数据不同罢了

Client:用来提交Spark程序的机器:这台机器一般一定和Spark Cluster在同样的网络环境中,(Driver频繁和Executors通信)且器配置和普通的Worker一致,千万不要把ide中直接提交app运行,会出现数据丢失,安全问题等.可能会有java ee等,Application(各种依赖的外部资源,例如*.so,File),使用spark-submit去运行程序(可以配置运行时app的各种参数例如memory,core..等等),实际生成环境下写脚本自动化配置和提交程序,当然当前一定要安装spark,只不过这里安装的spark不属于集群

Driver:核心是SparkContext,另外spark1.6内部实现通过rpc,底层还是akka,Driver在standalone模式下,不能作Ha,在Cluster模式下可以使用--supervise做ha(重启Driver).
	SparkContext最重要的工作:创建DAGScheduler, TaskScheduler, 和SchedulerBackend.在实例化的过程中,Register当前程序给Master,Master接受注册,如果没有问题,Master会为当前程序分配AppId,并分配计算资源

Spark Cluster:
	Master:接受用户提交程序并发送指令给Worker,为当前程序分配计算资源,每个Worker所在节点默认为当前程序分配一个Executor,在Executor中通过线程池并发执行,读取信息spark-env.sh,spark-default.sh,spark-submit提供的参数,程序中sparkconf配置的参数
	Master通知Worker按照要求启动Executor
	Worker Node:Worker进程:通过一个Proxy为ExectorRunner的对象实例来远程启动一个ExecutorBackend进程
	ExecutorBackend进程:里面有Executor,Executor内有线程池ThreadPool
	实际在工作的时候会通过TaskRunner(TaskRunner:Runnable接口,有run方法)来封装Task,然后从ThreadPool中获取一条线程执行Task,执行完后,线程被回收复用
	DAGScheduler:一般情况下,当通过action触发job时SparkContext会通过DAGScheduler来把Job中的RDD构成的DAG划分成不同的Stage,每个Stage内部是一系列业务逻辑完全相同但处理数据不同的Tasks,构成了TaskSet,TeskSet底层交给TaskScheduler(接口,抽象类),Task有两种类型:最后一个Stage中的Task成为ResultTask,产生Job的结果,其他前面的Stage的Task都是ShuffleMapTask,为下一阶段的Stage做数据准备,相当于MapReduce的Mapper
	TaskScheduler和TaskSchedulerBackend负责具体Task的运行(遵循数据本地性)

总结:
整个spark程序的运行,就是DAGScheduler把Job划分成不同的Stage,提交TaskSet给TaskScheduler,进而提交给Executor执行(符合数据本地性),每个Task会计算RDD中的一个Partition,基于该Partition来具体执行我们定义的一系列同一个Stage内部的函数,以此类推...,直到整个程序运行完成

--------------------------------------------------------------------
Hadop的MapReduce基于数据集,Spark的RDD基于工作集的应用抽象
位置感知,容错,负载均衡.
基于数据集的处理:从物理存储上加载数据,然后操作数据,然后写入物理存储设备.经典代表:Hadoop Mapreduce,也是DAG
基于数据集操作不适合的场景:不适合大量的迭代.不适合交互式查询,重点是基于数据流的方式不能够复用曾经的结果或者中间计算结果.
RDD:(Resilient Distributed Dataset)
	弹性第五点:checkpoint和persist(检查还原点,持久化),
	弹性第六点:数据调度弹性:DAG,Task和资源管理无关,
	弹性第七点:数据分片的高度弹性,(提高并行度和降低并行度),repartition合并和拆分
  /**
   * Return a new RDD that has exactly numPartitions partitions.
   * Can increase or decrease the level of parallelism in this RDD. Internally, this uses
   * a shuffle to redistribute data.
   *
   * If you are decreasing the number of partitions in this RDD, consider using `coalesce`,
   * which can avoid performing a shuffle.
   */
def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    coalesce(numPartitions, shuffle = true)
  }
一个stage有1000个rdd的transportation,默认产生一个中间结果
rdd是分布式函数式编程的抽象,源码说明如何形成rdd的依赖和lazy如下
  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.flatMap(cleanF))
  }
(插入:1.3版本后,datafream,可以对任意级别数据计算,shuffle可定制)
rdd容错的方式:数据检查点和记录数据的更新,数据检查点通过网络copy,网络IO是瓶颈.记录数据更新:复杂(每次记录都要更新),耗费性能
而spark RDD是基于记录数据更新
	1.RDD是不可变的链条,Lazy级别-->计算从后往前回溯
	2.对RDD写(更新)操作和改变都是粗粒度的,RDD的读操作既可以是粗粒度也可以是细粒度的-->效率,(简化复杂度)-->限制了RDD的使用场景,例如网络爬虫,而数据挖掘,机器学习等大多数场景都是粗粒度的
	3.RDD操作(compute,map...等)返回值接口化,this.type通用性,可扩展性,要做一体化,多元化,兼容一切平台,容器.但是,实时事务性处理取代不了,响应速度(事务性复杂)
	不支持:细粒度的更新操作,增量迭代计算(粗粒度原因),
	spark stream可以取代storm

