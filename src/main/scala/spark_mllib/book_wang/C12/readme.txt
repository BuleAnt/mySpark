TF-IDF 是一种简单的特征提取算法,一般认为:
	一盘文章的关键词是在其文章中出现最多的次,因此关键词提取一个最简单的思路就是提取文章出现最多的次,
	即词频(Term Frequency,TF)的提取.
	但文章中有些词作为常用词广泛使用,提取词频会产生大量的干扰噪音,解决办法为:
	当一篇文章中提取的词频较多的关键词在当前文章中多次出现,而在其他文章中较少出现,
	那么它可能最大幅度第反应了这篇文章的中心思想,即所谓的关键词.

用统计语言表示,对所有提取的每个词可以分配一个权重表示其重要性程度,一般情况下,
	常见词作为关键词所分配的权重较小,而不常见的词作为关键词分配的权重较大.
	这个权重叫做"逆文档频率"(Inverse Document Frequency,IDF),它的大小与一个词的常见程度成反比.

概括起来说,TF-IDF的一般定义如下:
	TF(Term Frequency)为词频的定义,表示某个关键词在一个文本中出现的次数,
		一般认为某个特定次在当前文本中出现的次数越多,越能反映出文本特征.
	IDF(Inverse Document Frequency)为你文本频率定义,表示为某个关键词在一个文本及中的区分能力.
		若某个特定关键词在文本集中出现的次数越多,则其区分能力越差.
		例如一些常用的介词完全没有任何区分能力,反而出现次数最多
TF-IDF算法数学计算:
	TF= 某个词在文章中出现的次数/文章的总次数
	IDF = log (查找的文章总数/(包含盖茨的文章数+1))
	从IDF中可以看到,一个词如果在不同文章中出现的较多,即较为常见,则可认为其坟墓越大,计算得到的IDF值越小.
	分母加1是为了防止-造成的计算错误.因此,最终TF-IDF计算公式:
		TF-IDF=TF(词频)*IDF(逆文档频率)

还需要注意的是,对不同的文本信息,经过TF-IDF确定的关键词向量后,其中可能包含较多数目的特征关键词,
因此选取不同数目的可欣关键词会对结果造成一定程度的影响.
一般认为,选取的关键词偏少,代表的信息熵不足;
过多的话,则可能会给关键词向量引入较多的噪声项,降低文本信息相似度计算的准确性


除此之外,需要说明的是,TF-IDF在实际使用过程中需要对文本进行分词处理.
这里建议采用中国科学院的ICTCLAS(http://www.ictclas.org)作为确定的粉刺工具,它主要有连个:
去除停用词,对提取的关键词做语义重构.
(1) 去除关键词的作用主要是去除一些常用的辅助词,这些词的存在不会对文章的意义产生任何影响.
	例如,常用的副词,介词,以及设定的一些文本中出现的特定地名,单位或组织机构名称等.
	以便于在对文本进行特征选择时,将其中忽略而避免对特征向量的建立产生影响.
(2)针对中文的使用特性,需要对提取的关键词做语义重构.
	由于中文文章中一般会出现较多有普通名词构成的专有名词.
	但在语义分析时,例如"数据挖掘"和"数据结构"这两个不同的词语,表示两个完全不同的学科.
	但是在语义分析时,分词器往往由于规则设定的不同,将其拆分为:"数据","挖掘","数据","结构"四个词.
	这洋在后续的分析中,由于完全不同的两个文本被标记成具有50%相似度的文本,这是非常严重的一个错误.
	因此必须对设定规则进行重构,区分不同的概念.

----------------------------------------------------------
词量化工具
简单的说,现实中语言文本问题要转化为机器学习或数据挖掘的问题,
第一步肯定是找一种方法把这些符号数字化,即要讲语言文本翻译成机器能够认识的语言.
词向量工具就是为了解决这个翻译问题而诞生的.
MLlib中提供了词向量化的工具,其目的是在于不增加位数的前提下讲大量的文本内容数字化.

Mllib中词向量砖汉采用的是skip-gram模型实现的,这个也是神经网络学习方法的一个特定学习方法,
skip-gram

基于卡方检验的特征选择

卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，
实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，
卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，
若两个值完全相等时，卡方值就为0，表明理论值完全符合。
（1）提出原假设：
H0：总体X的分布函数为F(x).
基于皮尔逊的检验统计量：

理解：n次试验中样本值落入第i个小区间Ai的频率fi/n与概率pi应很接近，当H0不真时，则fi/n与pi相差很大。在假设成立的情况下服从自由度为k-1的卡方分布。











