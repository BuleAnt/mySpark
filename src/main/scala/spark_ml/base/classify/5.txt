分类：分类的意义
传统意义下的分类：生物物种
预测：天气预报
决策：yes or no
分类的传统模型
分类（判别分析）与聚类有什么差别？分类有学习集，聚类没有学习集，而是根据几何形状进行划分。
有监督学习，无监督学习，半监督学习

常见分类模型与算法
	线性判别法
	距离判别法
	贝叶斯分类器
	决策树
	支持向量机(SVM)
	神经网络

一、线性判别法
	举例：天气预报数据
	> G=c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)  #1是晴天，2是雨天
	> x1=c(-1.9,-6.9,5.2,5.0,7.3,6.8,0.9,-12.5,1.5,3.8,0.2,-0.1,0.4,2.7,2.1,-4.6,-1.7,-2.6,2.6,-2.8)  #X1是湿度
	> x2=c(3.2,0.4,2.0,2.5,0.0,12.7,-5.4,-2.5,1.3,6.8,6.2,7.5,14.6,8.3,0.8,4.3,10.9,13.1,12.8,10.0)  #X2是温度
	> a=data.frame(G,x1,x2)
	> plot(x1,x2)
	> text(x1,x2,G,adj=-0.5)
	可以看出1类和2类数据的分离性非常明显

线性判别法的原理
用一条直线来划分学习集（这条直线一定存在吗？），然后根据待测点在直线的哪一边决定它的分类
R解决方案----MASS包与线性判别函数lda()
	> library(MASS)
	> ld=lda(G~x1+x2)
	> ld
	Call:
	lda(G ~ x1 + x2)

	Prior probabilities of groups:     #先验概率，计算分类1和2在总体中占的比例
	  1   2
	0.5 0.5

	Group means:    #两个因变量在两种分类中的均值
	     x1   x2
	1  0.92 2.10
	2 -0.38 8.85

	Coefficients of linear discriminants:   #求解的模型，包括X1和X2的系数
	          LD1
	x1 -0.1035305
	x2  0.2247957

	> z=predict(ld)   #用ld模型做预测，结果放入对象z中
	> newG=z$class    #z的属性class存放的是分类的结果
	> newG
	[1] 1 1 1 1 1 2 1 1 1 1 2 2 2 2 1 2 2 2 2 2
	Levels: 1 2
	> y=cbind(G,z$x,newG)   #打印原分类G，和模型中算出来的分类newG，LD1为判别函数的值，即当LD1<0时，newG将其归为第1类，反之，归为第2类
	> y
	   G         LD1        newG
	1  1 -0.28674901    1
	2  1 -0.39852439    1
	3  1 -1.29157053    1
	4  1 -1.15846657    1
	5  1 -1.95857603    1
	6  1  0.94809469    2
	7  1 -2.50987753    1
	8  1 -0.47066104    1
	9  1 -1.06586461    1
	10 1 -0.06760842    1
	11 2  0.17022402    2
	12 2  0.49351760    2
	13 2  2.03780185    2
	14 2  0.38346871    2
	15 2 -1.24038077    1
	16 2  0.24005867    2
	17 2  1.42347182    2
	18 2  2.01119984    2
	19 2  1.40540244    2
	20 2  1.33503926    2


二、距离判别法
KNN k最近邻法 k nearest neighbor
判断某个点类别时，找出离它最近的k（k通常取奇数）个点，k个临近点中属于哪个类的数据点多，就将其归入那个类。

三、贝叶斯分类器---基于条件概率的
用于二分类问题，分别计算样本点属于两类的概率，谁的概率高，点就归于这一类。

应用：文本挖掘典型场景
	网页自动分类
	垃圾邮件判断
	评论自动分析
	通过用户访问内容判别用户喜好

贝叶斯公式
	设X--包含某种特征的特定的样本，例如一封具体的邮件；H---假设，例如假设邮件为垃圾邮件
	P(H|X)---在具体观测样本X是某种假设的可能性，例如某封邮件是垃圾邮件的概率，也称为后验概率，即在条件X下，H发生的概率。
	P(H)---在总体样本中，发生假设事件的概率，例如所有邮件中垃圾邮件的概率，也称为先验概率。它之和总体样本分布有关，而和样本抽样无关。大数定理保证抽样数越多，概率和先验概率越接近。
	P(X)---X中特征出现的概率，X的先验概率，例如垃圾邮件中出现某个特征词的概率。

	P(H|X)=[P(X|H)*P(H)]/P(X)

	例如，拿到一个邮件是垃圾邮件的概率=在垃圾邮件中包含特征词的概率*垃圾邮件占总体邮件的概率/特征词在总体中出现的概率

	朴素贝叶斯：设X样本包含了X1、X2、X3三个特征，且三者相互独立，则近似P(X|H)=P(X1|H)*P(X2|H)*P(X3|H)，P(X)=P(X1)*P(X2)*P(X3)
	参见《数据挖掘：概念与技术（第三版）》.pdf P251,8.3.2

贝叶斯信念网络
	针对问题：朴素贝叶斯中特征互相独立的前提在很多情况下，难以满足

	表示一个特征变量，由于各变量不独立，所以用箭头表示其关系，箭头方向由业务专家指定。
	朴素贝叶斯分类器需要特征之间互相独立的强条件，制约了模型的适用
	用有向无环图表达变量之间的依赖关系，变量用节点表示，依赖关系用边表示。
	祖先，父母和后代节点。贝叶斯网络中的一个节点，如果它的父母节点已知，没有其它边和它链接，则它条件独立于它的所有非后代节点
	每个节点附带一个条件概率表（CPT），表示该节点和父母节点的联系概率。

从CPT中基于父母节点的条件概率推出某节点（变量）的概率，即以父母节点的取值组合作为条件而求得的概率
对于没有父母节点的祖先（例如图中的锻炼和饮食），直接写它的先验概率。

CPT表中的数据如何得出
	方一：询问专家
	方二：调查
	方三：由学习数据集统计

根据贝叶斯全概率公式，计算每个节点的概率
	例如，计算图中心脏病的先验概率：
	其中α和β分别为锻炼和饮食节点可能的取值。

基于后代节点的条件概率
	利用CPT，也可以反方向推出条件概率，即由后代节点向前推，
	例如，设A为心脏病，B为高血压，求已知高血压，求得心脏病的概率，即求P(A|B)
	则P(B|A)可以由图中CPT表查出来。
	而由贝叶斯公式可知，P(A|B)P(B)=P(B|A)P(A)=P(AB)
	所以P(A|B)=P(B|A)P(A)/P(B)
	同时基于父母节点和后代节点的条件概率

	例如，求图中在血压高、饮食健康、注意锻炼的前提下，得心脏病的概率。
	其它非父母，非后代节点与该节点本身是条件独立的概率
	例如 求图中心脏病和心口疼的条件概率
	设A为心脏病，B为心口疼，因为A、B相互独立
	则P(A|B)P(B)=P(AB)=P(A)P(B),
	所以P(A|B)=P(A)

也就是说，节点相互独立时的条件概率就等于先验概率

基于CPT图，可以算出想了解的所有情况的概率-----贝叶斯推理。
如果有缺失数据和隐藏变量时，CPT表中就会有空缺，而只要有学习数据，就可以通过算法填补这些空缺。

参考:
http://f.dataguru.cn/thread-604812-1-1.html