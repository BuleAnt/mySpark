	最大边缘超平面（MMH）
	向量xi和样本值yi已知，共有N个约束。向量w和截距b为待求变量。
	凸优化问题
	可以寻求凸优化算法支持解决
	可以使用拉格朗日乘子法继续简化

首先构造拉格朗日函数：
	由于约束取的是≥，所以该问题实际上是KKT条件。
	原问题可转变为求拉格朗日函数极值的问题：
	即求L的梯度▽L=0，即L变化最大处。
	进一步简化为对偶问题
	前一步得出的KKT条件中的变量太多
	为后续引入核函数作模型准备
	将前一步的梯度计算结果重新代入到拉格朗日函数：

	可见代入后，消除了w和b，变量只剩下拉格朗日乘子ai和aj的函数。------称为原拉格朗日函数的对偶问题。
	这时，变量减少了，也为向非线性情况的推广做了准备。
	比之前的凸优化问题简洁
	可以用各种凸优化算法加以解决
	只有支持向量参不计算，所以计算规模进低于我们的想象。

对偶公式中的未知数仅涉及拉格朗日乘子，而原问题中未知数还包含决策边界几何特征参数，未知数太多
待定乘子中实质有很多为0，仅在“支持向量”处不为0，所以最后的出的函数表达式进比想象中简单（但问题是预先无法知道哪些样本点是“支持向量”）

推广的SVM：用于存在线性不可分情况
	松弛变量与惩罚函数
	公式中蓝色的部分为在线性可分问题的基础上加上的惩罚函数部分，当xi在正确一边的时候，ε=0，R为全部的点的数目，C是一个由用户去指定的系数，表示对分错的点加入多少的惩罚，当C很大的时候，分错的点就会更少，但是过拟合的情况可能会比较严重，当C很小的时候，分错的点可能会很多，不过可能由此得到的模型也会不太正确，所以如何选择C是有很多学问的，很过在大部分情况下就是通过经验尝试得到的。εi全都是0时，即为线性可分的情况。
	该凸优化问题中w、b、ε都是待求变量。
	松弛变量的几何意义：与分错的点到分离平面的距离成正比。
	线性不可分时的对偶问题：



对于以上对偶问题，SMO算法为一个较高效的数值解法：
	Sequential minimal optimization
	Microsoft Research的JohnC. Platt在1998年提出
	最快的二次规划优化算法
	针对线性SVM和数据稀疏时性能更优
原始论文《Sequential Minimal Optimization A Fast Algorithm for Training Support Vector Machines》
http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html

基本思路是每次只更新两个乘子，迭代获得最终解。
非线性情况：映射至高维空间
	左图为红、蓝点在二维空间不可分情况，右图利用映射关系z=sqrt(x^2+y^2)将低维（2维）空间映射到高维（3维）空间，这样红色的点在z轴上会低一些，而蓝色的点会高一些，这样就变成了线性可分的情况，用一个平面就可以把它们分开。


公式中红字的地方要使用映射后的样本向量代替做内积
	最初的特征是n维的，我们将其映射到n^2维，然后再计算，这样需要的时间从原先的O（n）变成O（n^2），出现维数灾难。

通过引入核函数来解决维数灾难。
参考：http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988406.html
无须构造映射关系ψ，只要在低维空间能找到一个合理的核函数K，能将低维空间的非线性映射到高维空间的线性即可。
用Mercer定理判断构造的核函数是否有效：
Mercer定理：
如果函数K是
上的映射（也就是从两个n维向量映射到实数域）。那么如果K是一个有效核函数（也称为Mercer核函数），那么当且仅当对于训练样例
，其相应的核函数矩阵是对称半正定的。
核函数的实质就是逼近，类似于泰勒级数和傅里叶级数之类。

在拉格朗日函数中使用核函数简化：

待求参数仍然为拉格朗日乘子αi，i=1,...，n
求出αi后，进一步带到凸优化问题中去，求出法向量w和截距b，最终解决SVM问题。

注意，使用核函数后，怎么分类新来的样本呢？线性的时候我们使用SVM学习出w和b，新来样本x的话，我们使用
来判断，如果值大于等于1，那么是正类，小于等于是负类。在两者之间，认为无法确定。如果使用了核函数后，
就变成了
，是否先要找到
，然后再预测？答案肯定不是了，找
很麻烦，回想我们之前说过的

   第一步即w用拉格朗日极值问题替代。

只需将
替换成核函数
，然后值的判断同上。

R中实验SVM
	使用e1071包
	> library(e1071)
	> attach(iris)
	> ## classification mode
	> # default with factor response:
	> model <-svm(Species ~ ., data = iris)
	> # alternatively the traditional interface:
	> x <-subset(iris, select = -Species)
	> y <-Species
	> model <-svm(x, y)
	> print(model)

	Call:
	svm.default(x = x, y = y)


	Parameters:
	   SVM-Type:  C-classification
	SVM-Kernel:  radial
	       cost:  1
	      gamma:  0.25

	Number of Support Vectors:  51

	> summary(model)

	Call:
	svm.default(x = x, y = y)


	Parameters:
	   SVM-Type:  C-classification
	SVM-Kernel:  radial
	       cost:  1
	      gamma:  0.25

	Number of Support Vectors:  51

	( 8 22 21 )


	Number of Classes:  3

	Levels:
	setosa versicolor virginica

	> # test with train data
	> pred<-predict(model, x)
	> # (same as
	> pred<-fitted(model)
	> # Check accuracy:
	> table(pred, y)
	            y
	pred         setosa versicolor virginica
	  setosa         50          0         0
	  versicolor      0         48         2
	  virginica       0          2        48
	> # compute decision values and probabilities:
	> pred<-predict(model, x, decision.values= TRUE)
	> attr(pred, "decision.values"[1:4,]
	  setosa/versicolor setosa/virginica versicolor/virginica
	1          1.196152         1.091757            0.6708810
	2          1.064621         1.056185            0.8483518
	3          1.180842         1.074542            0.6439798
	4          1.110699         1.053012            0.6782041
	> # visualize (classes by color, SV by crosses):
	> plot(cmdscale(dist(iris[,-5])),
	+ col= as.integer(iris[,5]),
	+ pch= c("o","+")[1:150 %in% model$index+ 1])

svm系列
	线性支持向量机
	http://www.cnblogs.com/pinard/p/6097604.html
	线性支持向量机的软间隔最大化模型
	http://www.cnblogs.com/pinard/p/6100722.html
	线性不可分支持向量机与核函数
	http://www.cnblogs.com/pinard/p/6103615.html
	SMO算法原理
	http://www.cnblogs.com/pinard/p/6111471.html
	线性支持回归
	http://www.cnblogs.com/pinard/p/6113120.html
svm支持向量机高斯核调参
http://www.cnblogs.com/pinard/p/6126077.html
SVM原理
http://blog.csdn.net/u014688145/article/details/52906162
http://blog.csdn.net/u014688145/article/details/52943923
svm实现
http://blog.csdn.net/u014688145/article/details/52963915


参考:
关于支持向量机：http://www.cnblogs.com/LeftNotEa ... /05/18/2034566.html
关于拉格朗日乘子法：http://blog.csdn.net/xianlingmao/article/details/7919597
关于KKT条件：http://hi.baidu.com/grandyang/item/94cd68dfdc06941e21e25099
求解凸优化问题的斱法：http://wenku.baidu.com/link?url= ... KExJtvVTS3Uj4S68UpG

http://f.dataguru.cn/thread-611098-1-1.html
